{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "add_dir = os.path.abspath('./data/')\n",
    "sys.path.append(add_dir)\n",
    "import tf_extended as tfe\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.python.ops import random_ops,control_flow_ops\n",
    "from tensorflow.python.keras.initializers import he_normal\n",
    "import math\n",
    "import random\n",
    "from data import ssd_common\n",
    "from data import loss_function\n",
    "from matplotlib import pyplot as plt\n",
    "from data import custom_layers\n",
    "import tensorlayer as tl\n",
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_integer('image_size', 300, \"Needs to provide same value as in training.\")\n",
    "tf.app.flags.DEFINE_integer('batch_size', 16, \"Batch size for training.\")\n",
    "tf.app.flags.DEFINE_integer('num_class', 21, \"Actual num of class +1.\")\n",
    "tf.app.flags.DEFINE_string('log_dir','./SSD_Billy/log','tensorboard directory')\n",
    "tf.app.flags.DEFINE_string('checkpoint_dir','./SSD_Billy/checkpoint/','The directory where to save the parameters of the network')\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip(image):\n",
    "\n",
    "    rand_num = random.random()*2-1\n",
    "    if(rand_num <(-1/3)):\n",
    "        return cv2.flip(image, -1)\n",
    "    elif(rand_num>(-1/3) and rand_num < (1/3)):\n",
    "        return cv2.flip(image, 0)\n",
    "    elif(rand_num>(1/3)):\n",
    "        return cv2.flip(image, 1)\n",
    "    else:\n",
    "        return cv2.flip(image, rand_num)\n",
    "    \n",
    "def random_crop(image):\n",
    "    rate = (random.random()+1)*0.1*0.5 #random crop 10% to 20%\n",
    "    cropImg = image[int(image.shape[0]*rate):int(image.shape[0]*(1-rate)),int(image.shape[1]*rate):int(image.shape[1]*(1-rate))]\n",
    "    return cropImg\n",
    "    \n",
    "def random_rotate_image(image):\n",
    "    #random rotate\n",
    "    (h,w) = image.shape[:2]\n",
    "    center = (w//2,h//2)\n",
    "    M = cv2.getRotationMatrix2D(center,random.random()*360,1.0)\n",
    "    image = cv2.warpAffine(image,M,(w,h),borderMode = cv2.BORDER_REPLICATE)\n",
    "    return image\n",
    "\n",
    "def random_distort_image(image, hue=18, saturation=1.5, exposure=1.5):\n",
    "    def _rand_scale(scale):\n",
    "        scale = np.random.uniform(1, scale)\n",
    "        return scale if (np.random.randint(2) == 0) else 1. / scale\n",
    "\n",
    "    # determine scale factors\n",
    "    dhue = np.random.uniform(-hue, hue)\n",
    "    dsat = _rand_scale(saturation)\n",
    "    dexp = _rand_scale(exposure)\n",
    "    # convert RGB space to HSV space\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV).astype('float')\n",
    "    # change satuation and exposure\n",
    "    image[:, :, 1] *= dsat\n",
    "    image[:, :, 2] *= dexp\n",
    "    # change hue\n",
    "    image[:, :, 0] += dhue\n",
    "    image[:, :, 0] -= (image[:, :, 0] > 180) * 180\n",
    "    image[:, :, 0] += (image[:, :, 0] < 0) * 180\n",
    "    \n",
    "    # avoid overflow when astype('uint8')\n",
    "    image[...] = np.clip(image[...], 0, 255)\n",
    "    # convert back to RGB from HSV\n",
    "    return cv2.cvtColor(image.astype('uint8'), cv2.COLOR_HSV2RGB)\n",
    "\n",
    "def data_augmentation(image):\n",
    "    image = random_crop(image)\n",
    "    image = random_flip(image)\n",
    "    image = random_rotate_image(image)\n",
    "    image = random_distort_image(image, hue=18, saturation=1.5, exposure=1.5)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training=False):\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=True, activation=None)\n",
    "    layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer_conv(prev_layer, layer_depth, is_training=False):\n",
    "    conv_layer1 = tf.layers.conv2d(prev_layer, layer_depth, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "    conv_layer1_bn = tf.layers.batch_normalization(conv_layer1, training=is_training)\n",
    "    conv_layer1_out = tf.nn.relu(conv_layer1_bn)\n",
    "\n",
    "    pool_layer1 = tf.layers.max_pooling2d(conv_layer1_out,[2,2],strides=2,padding='same')\n",
    "    return pool_layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer_2conv(prev_layer, layer_depth, is_training=False):\n",
    "    conv_layer1 = tf.layers.conv2d(prev_layer, layer_depth, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "    conv_layer1_bn = tf.layers.batch_normalization(conv_layer1, training=is_training)\n",
    "    conv_layer1_out = tf.nn.relu(conv_layer1_bn)\n",
    "    \n",
    "    conv_layer2 = tf.layers.conv2d(conv_layer1_out, layer_depth, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "    conv_layer2_bn = tf.layers.batch_normalization(conv_layer2, training=is_training)\n",
    "    conv_layer2_out = tf.nn.relu(conv_layer2_bn)\n",
    "\n",
    "    pool_layer2 = tf.layers.max_pooling2d(conv_layer2_out,[2,2],strides=2,padding='same')\n",
    "    return pool_layer2,conv_layer2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer_3conv(prev_layer, layer_depth, is_training=False):\n",
    "    conv_layer1 = tf.layers.conv2d(prev_layer, layer_depth, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "    conv_layer1_bn = tf.layers.batch_normalization(conv_layer1, training=is_training)\n",
    "    conv_layer1_out = tf.nn.relu(conv_layer1_bn)\n",
    "    \n",
    "    conv_layer2 = tf.layers.conv2d(conv_layer1_out, layer_depth, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "    conv_layer2_bn = tf.layers.batch_normalization(conv_layer2, training=is_training)\n",
    "    conv_layer2_out = tf.nn.relu(conv_layer2_bn)\n",
    "    \n",
    "    conv_layer3 = tf.layers.conv2d(conv_layer2_out, layer_depth, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "    conv_layer3_bn = tf.layers.batch_normalization(conv_layer3, training=is_training)\n",
    "    conv_layer3_out = tf.nn.relu(conv_layer3_bn)\n",
    "    pool_layer3 = tf.layers.max_pooling2d(conv_layer3_out,[2,2],strides=2,padding='same')\n",
    "    return pool_layer3,conv_layer3_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(anchor_sizes[0])+len(anchor_ratios[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_multibox_layer(layer,anchor_sizes,anchor_ratios,feat_shapes,normalization=False):\n",
    "    #need to figure out why length of anchor_size + length of anchor_ratios\n",
    "    if normalization > 0:\n",
    "        layer = custom_layers.l2_normalization(layer, scaling=True)\n",
    "    num_anchors = len(anchor_sizes)+len(anchor_ratios)\n",
    "    num_loc_pred = num_anchors *4\n",
    "    num_cls_pred = num_anchors * FLAGS.num_class\n",
    "    \n",
    "    pred = tf.layers.conv2d(layer, num_loc_pred+num_cls_pred, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "    print(pred.shape)\n",
    "    loc_pred = tf.reshape(pred[...,0:(4*num_anchors)],[-1,feat_shapes[0],feat_shapes[0], num_anchors,4])\n",
    "    \n",
    "#     cls_pred = tf.layers.conv2d(layer, num_cls_pred, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "    cls_pred = tf.reshape(pred[...,(4*num_anchors)::],[-1,feat_shapes[0],feat_shapes[0],num_anchors,FLAGS.num_class])\n",
    "    #channel to last and reshape didn't implement\n",
    "    return cls_pred,loc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_anchor_one_layer(img_shape,#原始图像shape\n",
    "                         feat_shape,#特征图shape\n",
    "                         sizes,#预设的box size\n",
    "                         ratios,#aspect 比例\n",
    "                         step,#anchor的层\n",
    "                         offset=0.5,\n",
    "                         dtype=np.float32):\n",
    "    \"\"\"Computer SSD default anchor boxes for one feature layer.\n",
    "\n",
    "    Determine the relative position grid of the centers, and the relative\n",
    "    width and height.\n",
    "\n",
    "    Arguments:\n",
    "      feat_shape: Feature shape, used for computing relative position grids;\n",
    "      size: Absolute reference sizes;\n",
    "      ratios: Ratios to use on these features;\n",
    "      img_shape: Image shape, used for computing height, width relatively to the\n",
    "        former;\n",
    "      offset: Grid offset.\n",
    "\n",
    "    Return:\n",
    "      y, x, h, w: Relative x and y grids, and height and width.\n",
    "    \"\"\"\n",
    "    # Compute the position grid: simple way.\n",
    "    # y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]\n",
    "    # y = (y.astype(dtype) + offset) / feat_shape[0]\n",
    "    # x = (x.astype(dtype) + offset) / feat_shape[1]\n",
    "    # Weird SSD-Caffe computation using steps values...\n",
    "    \n",
    "    \"\"\"\n",
    "    #测试中，参数如下\n",
    "    feat_shapes=[(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)]\n",
    "    anchor_sizes=[(21., 45.),\n",
    "                      (45., 99.),\n",
    "                      (99., 153.),\n",
    "                      (153., 207.),\n",
    "                      (207., 261.),\n",
    "                      (261., 315.)]\n",
    "    anchor_ratios=[[2, .5],\n",
    "                       [2, .5, 3, 1./3],\n",
    "                       [2, .5, 3, 1./3],\n",
    "                       [2, .5, 3, 1./3],\n",
    "                       [2, .5],\n",
    "                       [2, .5]]\n",
    "    anchor_steps=[8, 16, 32, 64, 100, 300]\n",
    "\n",
    "\n",
    "    offset=0.5\n",
    "\n",
    "    dtype=np.float32\n",
    "\n",
    "    feat_shape=feat_shapes[0]\n",
    "    step=anchor_steps[0]\n",
    "    \"\"\"\n",
    "    #测试中，y和x的shape为（38,38）（38,38）\n",
    "    #y的值为\n",
    "    #array([[ 0,  0,  0, ...,  0,  0,  0],\n",
    "     #  [ 1,  1,  1, ...,  1,  1,  1],\n",
    "    # [ 2,  2,  2, ...,  2,  2,  2],\n",
    "    #   ..., \n",
    "     #  [35, 35, 35, ..., 35, 35, 35],\n",
    "    #  [36, 36, 36, ..., 36, 36, 36],\n",
    "     #  [37, 37, 37, ..., 37, 37, 37]])\n",
    "    y, x = np.mgrid[0:feat_shape[0], 0:feat_shape[1]]\n",
    "    #测试中y=(y+0.5)×8/300,x=(x+0.5)×8/300\n",
    "    y = (y.astype(dtype) + offset) * step / img_shape[0]\n",
    "    x = (x.astype(dtype) + offset) * step / img_shape[1]\n",
    "\n",
    "    #扩展维度，维度为（38,38,1）\n",
    "    # Expand dims to support easy broadcasting.\n",
    "    y = np.expand_dims(y, axis=-1)\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "\n",
    "    # Compute relative height and width.\n",
    "    # Tries to follow the original implementation of SSD for the order.\n",
    "    #数值为2+2\n",
    "    num_anchors = len(sizes) + len(ratios)\n",
    "    #shape为（4,）\n",
    "    h = np.zeros((num_anchors, ), dtype=dtype)\n",
    "    w = np.zeros((num_anchors, ), dtype=dtype)\n",
    "    # Add first anchor boxes with ratio=1.\n",
    "    #测试中，h[0]=21/300,w[0]=21/300?\n",
    "    h[0] = sizes[0] / img_shape[0]\n",
    "    w[0] = sizes[0] / img_shape[1]\n",
    "    di = 1\n",
    "    if len(sizes) > 1:\n",
    "        #h[1]=sqrt(21*45)/300\n",
    "        h[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[0]\n",
    "        w[1] = math.sqrt(sizes[0] * sizes[1]) / img_shape[1]\n",
    "        di += 1\n",
    "    for i, r in enumerate(ratios):\n",
    "        h[i+di] = sizes[0] / img_shape[0] / math.sqrt(r)\n",
    "        w[i+di] = sizes[0] / img_shape[1] * math.sqrt(r)\n",
    "    #测试中，y和x shape为（38,38,1）\n",
    "    #h和w的shape为（4,）\n",
    "    return y, x, h, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_anchors_all_layers(img_shape,\n",
    "                           layers_shape,\n",
    "                           anchor_sizes,\n",
    "                           anchor_ratios,\n",
    "                           anchor_steps,\n",
    "                           offset=0.5,\n",
    "                           dtype=np.float32):\n",
    "    \"\"\"Compute anchor boxes for all feature layers.\n",
    "    \"\"\"\n",
    "    layers_anchors = []\n",
    "    for i, s in enumerate(layers_shape):\n",
    "        anchor_bboxes = ssd_anchor_one_layer(img_shape, s,\n",
    "                                             anchor_sizes[i],\n",
    "                                             anchor_ratios[i],\n",
    "                                             anchor_steps[i],\n",
    "                                             offset=offset, dtype=dtype)\n",
    "        layers_anchors.append(anchor_bboxes)\n",
    "    return layers_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(img_list,annotation,batch_size,aug):\n",
    "    num_batch = len(img_list)/batch_size\n",
    "    count=0\n",
    "    while(True):\n",
    "        image_data = []\n",
    "        annotation_data = []\n",
    "        for i in range(batch_size):\n",
    "            temp_index = i+count*batch_size\n",
    "            temp_index %=len(img_list) \n",
    "            image = cv2.imread(img_list[temp_index])\n",
    "            if aug:\n",
    "                image = data_augmentation(image)\n",
    "            image = image[:,:,::-1]\n",
    "            image = image.astype(np.float32)\n",
    "            \n",
    "#             image = cv2.resize(image,(FLAGS.image_size,FLAGS.image_size))\n",
    "            image = image/255\n",
    "            image_data.append(image)\n",
    "            annotation_data.append(annotation[temp_index])\n",
    "        count+=1\n",
    "#         image_data = np.array(image_data)\n",
    "        yield image_data,annotation_data\n",
    "\n",
    "def get_path_and_annotation(file_path):\n",
    "    annotation = []\n",
    "    img_path = []\n",
    "    line_list = []\n",
    "    with open(file_path,'r') as f:\n",
    "        for line in f:\n",
    "            temp=[]\n",
    "            line = line.strip('\\n')\n",
    "            line_list.append(line)\n",
    "        random.shuffle(line_list)\n",
    "\n",
    "    for i in line_list:\n",
    "        line = i.split(' ')\n",
    "        img_path.append(line[0])\n",
    "        temp = []\n",
    "        temp_inner= []\n",
    "        for j in range(1,len(line)):\n",
    "            temp.append(line[j].split(','))\n",
    "        annotation.append(temp)\n",
    "    return img_path,annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img_bbox(original_img,target_size,annotation):\n",
    "    xmin_resized = []\n",
    "    ymin_resized = []\n",
    "    xmax_resized = []\n",
    "    ymax_resized = []\n",
    "    ratio_list = []\n",
    "    img = cv2.resize(original_img,(target_size,target_size))\n",
    "    for i in annotation:\n",
    "        xmin = int(float(i[0]))\n",
    "        ymin = int(float(i[1]))\n",
    "        xmax = int(i[2])\n",
    "        ymax = int(i[3])\n",
    "\n",
    "        x_ratio = 300/original_img.shape[1]\n",
    "        y_ratio = 300/original_img.shape[0]\n",
    "        ratio_list.append([x_ratio,y_ratio])\n",
    "        xmin_resized.append(int(np.round(xmin*x_ratio)))\n",
    "        ymin_resized.append(int(np.round(ymin*y_ratio)))\n",
    "        xmax_resized.append(int(np.round(xmax*x_ratio)))\n",
    "        ymax_resized.append(int(np.round(ymax*y_ratio)))\n",
    "    return img,xmin_resized,ymin_resized,xmax_resized,ymax_resized,ratio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(image_batch,annotation_batch,cls_batch,target_size):\n",
    "\n",
    "    x_batch_list =list()\n",
    "    y_batch_list =list()\n",
    "    w_batch_list =list()\n",
    "    h_batch_list =list()\n",
    "    cls_batch_list = list()\n",
    "    image_batch_list = list()\n",
    "    anno_batch_list = list()\n",
    "    for i in range(len(annotation_batch)):\n",
    "        #loop for each img\n",
    "        temp_anno = list()\n",
    "        temp_cls = list()\n",
    "        image_batch_temp,xmin,ymin,xmax,ymax,_=resize_img_bbox(image_batch[i],target_size,annotation_batch[i])\n",
    "        \n",
    "        image_batch_list.append(image_batch_temp)\n",
    "        for j in range(len(annotation_batch[i])):\n",
    "            #loop for each bbox in one img\n",
    "            temp_anno.append([ymin[j]/300,xmin[j]/300,ymax[j]/300,xmax[j]/300])\n",
    "            temp_cls.append(int(cls_batch[i][j]))\n",
    "        for j in range(60-len(annotation_batch[i])):\n",
    "            temp_anno.append([0,0,0,0])\n",
    "            temp_cls.append(0)\n",
    "        anno_batch_list.append(np.array(temp_anno,dtype=np.float32))\n",
    "        cls_batch_list.append(np.array(temp_cls,dtype=np.int32))\n",
    "        \n",
    "    image_batch = np.array(image_batch_list,dtype=np.float32)\n",
    "#     anno_batch_list = np.array(anno_batch_list,dtype=np.float32)\n",
    "#     cls_batch_list = np.array(cls_batch_list,dtype=np.int32)\n",
    "    return image_batch,anno_batch_list,cls_batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_tensor(tensor):\n",
    "    final_result = []\n",
    "    for j in range(6):\n",
    "        temp_tensor = []\n",
    "        for i in range(FLAGS.batch_size):\n",
    "            temp_tensor.append(tensor[i][j])\n",
    "        final_result.append(tf.stack(temp_tensor,axis=0))\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_losses(logits, localisations,\n",
    "               gclasses, glocalisations, gscores,\n",
    "               match_threshold=0.5,\n",
    "               negative_ratio=3.,\n",
    "               alpha=1.,\n",
    "               label_smoothing=0.,\n",
    "               device='/cpu:0',\n",
    "               scope=None):\n",
    "    with tf.name_scope(scope, 'ssd_losses'):\n",
    "        lshape = tfe.get_shape(logits[0], 5)\n",
    "        num_classes = lshape[-1]\n",
    "        batch_size = lshape[0]\n",
    "\n",
    "        # Flatten out all vectors!\n",
    "        flogits = []\n",
    "        fgclasses = []\n",
    "        fgscores = []\n",
    "        flocalisations = []\n",
    "        fglocalisations = []\n",
    "        for i in range(len(logits)):\n",
    "            flogits.append(tf.reshape(logits[i], [-1, num_classes]))\n",
    "            fgclasses.append(tf.reshape(gclasses[i], [-1]))\n",
    "            fgscores.append(tf.reshape(gscores[i], [-1]))\n",
    "            flocalisations.append(tf.reshape(localisations[i], [-1, 4]))\n",
    "            fglocalisations.append(tf.reshape(glocalisations[i], [-1, 4]))\n",
    "        # And concat the crap!\n",
    "        logits = tf.concat(flogits, axis=0)\n",
    "        gclasses = tf.concat(fgclasses, axis=0)\n",
    "        gscores = tf.concat(fgscores, axis=0)\n",
    "        localisations = tf.concat(flocalisations, axis=0)\n",
    "        glocalisations = tf.concat(fglocalisations, axis=0)\n",
    "        dtype = logits.dtype\n",
    "\n",
    "        # Compute positive matching mask...\n",
    "        pmask = gscores > match_threshold\n",
    "        #################################someone on github#########\n",
    "#         pmask = tf.concat(axis=0, values=[pmask[:tf.argmax(gscores)],[True],pmask[tf.argmax(gscores)+1:]])\n",
    "        ###########################################################\n",
    "        fpmask = tf.cast(pmask, dtype)\n",
    "        n_positives = tf.reduce_sum(fpmask)\n",
    "\n",
    "        # Hard negative mining...\n",
    "        no_classes = tf.cast(pmask, tf.int32)\n",
    "        predictions = slim.softmax(logits)\n",
    "        nmask = tf.logical_and(tf.logical_not(pmask),\n",
    "                               gscores > -0.5)\n",
    "        fnmask = tf.cast(nmask, dtype)\n",
    "        nvalues = tf.where(nmask,\n",
    "                           predictions[:, 0],\n",
    "                           1. - fnmask)\n",
    "        nvalues_flat = tf.reshape(nvalues, [-1])\n",
    "        # Number of negative entries to select.\n",
    "        max_neg_entries = tf.cast(tf.reduce_sum(fnmask), tf.int32)\n",
    "        n_neg = tf.cast(negative_ratio * n_positives, tf.int32) + batch_size\n",
    "        n_neg = tf.minimum(n_neg, max_neg_entries)\n",
    "\n",
    "        val, idxes = tf.nn.top_k(-nvalues_flat, k=n_neg)\n",
    "        max_hard_pred = -val[-1]\n",
    "        # Final negative mask.\n",
    "        nmask = tf.logical_and(nmask, nvalues < max_hard_pred)\n",
    "        fnmask = tf.cast(nmask, dtype)\n",
    "\n",
    "        # Add cross-entropy loss.\n",
    "        with tf.name_scope('cross_entropy_pos'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                  labels=gclasses)\n",
    "            loss = tf.div(tf.reduce_sum(loss * fpmask), tf.cast(batch_size,dtype), name='value')\n",
    "            tf.losses.add_loss(loss)\n",
    "            cross_entropy_pos_loss = loss\n",
    "        with tf.name_scope('cross_entropy_neg'):\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                  labels=no_classes)\n",
    "            loss = tf.div(tf.reduce_sum(loss * fnmask), tf.cast(batch_size,dtype), name='value')\n",
    "            tf.losses.add_loss(loss)\n",
    "            cross_entropy_neg_loss = loss\n",
    "        # Add localization loss: smooth L1, L2, ...\n",
    "        with tf.name_scope('localization'):\n",
    "            # Weights Tensor: positive mask + random negative.\n",
    "            weights = tf.expand_dims(alpha * fpmask, axis=-1)\n",
    "            loss = custom_layers.abs_smooth(localisations - glocalisations)\n",
    "            loss = tf.div(tf.reduce_sum(loss * weights), tf.cast(batch_size,dtype), name='value')\n",
    "            tf.losses.add_loss(loss)\n",
    "            localization_loss = loss\n",
    "        return cross_entropy_pos_loss,cross_entropy_neg_loss,localization_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some constant values\n",
    "feat_layers=['block4', 'block7', 'block8', 'block9', 'block10', 'block11']\n",
    "feat_shapes=[(38, 38), (19, 19), (10, 10), (5, 5), (3, 3), (1, 1)]\n",
    "anchor_size_bounds=[0.15, 0.90]\n",
    "anchor_sizes=[(21., 45.),\n",
    "                      (45., 99.),\n",
    "                      (99., 153.),\n",
    "                      (153., 207.),\n",
    "                      (207., 261.),\n",
    "                      (261., 315.)]\n",
    "anchor_ratios=[[2, .5],\n",
    "                       [2, .5, 3, 1./3],\n",
    "                       [2, .5, 3, 1./3],\n",
    "                       [2, .5, 3, 1./3],\n",
    "                       [2, .5],\n",
    "                       [2, .5]]                                                                      \n",
    "anchor_steps=[8, 16, 32, 64, 100, 300]\n",
    "anchor_offset=0.5\n",
    "normalizations=[20, -1, -1, -1, -1, -1]\n",
    "prior_scaling=[0.1, 0.1, 0.2, 0.2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 38, 38, 100)\n",
      "(?, 19, 19, 150)\n",
      "(?, 10, 10, 150)\n",
      "(?, 5, 5, 150)\n",
      "(?, 3, 3, 100)\n",
      "(?, 1, 1, 100)\n"
     ]
    }
   ],
   "source": [
    "#####################the start of bottle neck##################\n",
    "inputs = tf.placeholder(tf.float32, [None, FLAGS.image_size, FLAGS.image_size, 3],name='inputs')\n",
    "labels = tf.placeholder(tf.int32, [None,None])\n",
    "bboxes = tf.placeholder(tf.float32, [None,None,4])\n",
    "# keep_prob = tf.placeholder(tf.float32)\n",
    "is_training = tf.placeholder(tf.bool,name='is_training')\n",
    "end_points={}\n",
    "layer = inputs \n",
    "#conv1\n",
    "layer,_ = conv_layer_2conv(layer, 64, is_training=is_training)\n",
    "#conv2\n",
    "layer,_ = conv_layer_2conv(layer, 128, is_training=is_training)\n",
    "#conv3\n",
    "layer,_ = conv_layer_3conv(layer, 256, is_training=is_training)\n",
    "#conv4\n",
    "layer,foot_stage1 = conv_layer_3conv(layer, 512, is_training=is_training)\n",
    "end_points['block4'] = foot_stage1\n",
    "#conv5\n",
    "layer = tf.layers.conv2d(layer, 512, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "layer = tf.nn.relu(layer)\n",
    "layer = tf.layers.conv2d(layer, 512, [3,3], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "layer = tf.layers.max_pooling2d(layer,[3,3],strides=1,padding='same')\n",
    "layer = tf.nn.relu(layer)\n",
    "\n",
    "#FC6\n",
    "layer = slim.conv2d(layer, 1024, [3, 3], rate=6, scope='conv6')\n",
    "\n",
    "#FC7\n",
    "layer = tf.layers.conv2d(layer, 1024, [1,1], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "layer = tf.nn.relu(layer)\n",
    "end_points['block7'] = layer\n",
    "\n",
    "#conv8\n",
    "layer = tf.layers.conv2d(layer, 256, [1,1], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "layer = tf.nn.relu(layer)\n",
    "layer = tf.layers.conv2d(layer, 512, [3,3], 2, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "layer = tf.nn.relu(layer)\n",
    "end_points['block8'] = layer\n",
    "\n",
    "#conv9\n",
    "layer = tf.layers.conv2d(layer, 128, [1,1], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "layer = tf.nn.relu(layer)\n",
    "layer = tf.layers.conv2d(layer, 256, [3,3], 2, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "layer = tf.nn.relu(layer)\n",
    "end_points['block9'] = layer\n",
    "\n",
    "#conv10\n",
    "layer = tf.layers.conv2d(layer, 128, [1,1], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "layer = tf.nn.relu(layer)\n",
    "layer = tf.layers.conv2d(layer, 256, [3,3], 1, 'valid', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "layer = tf.nn.relu(layer)\n",
    "end_points['block10'] = layer\n",
    "\n",
    "#conv11\n",
    "layer = tf.layers.conv2d(layer, 128, [1,1], 1, 'same', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "layer = tf.nn.relu(layer)\n",
    "layer = tf.layers.conv2d(layer, 256, [3,3], 1, 'valid', use_bias=True,kernel_initializer=he_normal(seed=0.01),activation=None,kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01))\n",
    "layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "layer = tf.nn.relu(layer)\n",
    "end_points['block11'] = layer\n",
    "\n",
    "\n",
    "\n",
    "predictions =[]\n",
    "logits = []\n",
    "localisations=[]\n",
    "for i,layer in enumerate(feat_layers):\n",
    "    with tf.variable_scope(layer + '_box'):\n",
    "        p, l = ssd_multibox_layer(end_points[layer],anchor_sizes[i],anchor_ratios[i],feat_shapes[i],normalizations[i])\n",
    "        predictions.append(tf.contrib.slim.softmax(p))\n",
    "        logits.append(p)########(layers,batch_size,featuremap,featuremap,boxes_layer,num_class)\n",
    "        localisations.append(l)#####(layers,batch_size,featuremap,featuremap,boxes_layer,4)\n",
    "        \n",
    "# generate bboxes for each feet layer\n",
    "anchor_layers_original = ssd_anchors_all_layers((FLAGS.image_size,FLAGS.image_size),\n",
    "                           feat_shapes,\n",
    "                           anchor_sizes,\n",
    "                           anchor_ratios,\n",
    "                           anchor_steps,\n",
    "                           offset=0.5,\n",
    "                           dtype=np.float32)\n",
    "\n",
    "################     encode bboxes to each layer      #################\n",
    "feat_labels = []\n",
    "feat_localizations = []\n",
    "feat_scores = []\n",
    "for i in range(FLAGS.batch_size):\n",
    "    feat_labels_temp, feat_localizations_temp, feat_scores_temp = ssd_common.tf_ssd_bboxes_encode(labels[i],bboxes[i],anchor_layers_original,FLAGS.num_class,0)\n",
    "    feat_labels.append(feat_labels_temp)\n",
    "    feat_localizations.append(feat_localizations_temp)\n",
    "    feat_scores.append(feat_scores_temp) \n",
    "\n",
    "feat_labels = stack_tensor(feat_labels)\n",
    "feat_localizations = stack_tensor(feat_localizations)\n",
    "feat_scores = stack_tensor(feat_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################This part is used for inference #########################\n",
    "decoded_boxes = ssd_common.tf_ssd_bboxes_decode(localisations,\n",
    "                         anchor_layers_original,\n",
    "                         prior_scaling=[0.1, 0.1, 0.2, 0.2],\n",
    "                         scope='ssd_bboxes_decode')\n",
    "\n",
    "def reshape_tensors(localisations_layer,logits_layer,predictions_layer):\n",
    "    localisations_layer = tf.reshape(localisations_layer,(1,-1,4))\n",
    "    logits_layer = tf.reshape(logits_layer,(1,-1,21))\n",
    "    predictions_layer = tf.reshape(predictions_layer,(1,-1,21))\n",
    "    return localisations_layer,logits_layer,predictions_layer\n",
    "\n",
    "reshaped_locals = []\n",
    "reshaped_logits = []\n",
    "reshaped_predictions = []\n",
    "for i in range(len(feat_layers)):\n",
    "    temp_locals,temp_logits,temp_predictions = reshape_tensors(decoded_boxes[i],logits[i],predictions[i])\n",
    "    reshaped_locals.append(temp_locals)\n",
    "    reshaped_logits.append(temp_logits)\n",
    "    reshaped_predictions.append(temp_predictions)\n",
    "reshaped_locals = tf.concat(reshaped_locals,axis=1)\n",
    "reshaped_logits = tf.concat(reshaped_logits,axis=1)\n",
    "reshaped_predictions = tf.concat(reshaped_predictions,axis=1)\n",
    "\n",
    "classes = tf.cast(tf.argmax(reshaped_predictions,axis=2),tf.int32)\n",
    "scores = tf.reduce_max(reshaped_predictions,axis=2)\n",
    "#remove boxes belongs to background\n",
    "scores = scores * tf.cast(classes >0, scores.dtype)\n",
    "#remove boxes with low scores\n",
    "mask = tf.greater(scores, 0.5)\n",
    "classes = classes * tf.cast(mask, classes.dtype)\n",
    "scores = scores * tf.cast(mask, scores.dtype)\n",
    "ymin = tf.zeros_like(reshaped_locals[...,0])\n",
    "xmin = tf.zeros_like(reshaped_locals[...,1])\n",
    "ymax = tf.ones_like(reshaped_locals[...,2])\n",
    "xmax = tf.ones_like(reshaped_locals[...,3])\n",
    "\n",
    "ymin = tf.maximum(reshaped_locals[...,0],ymin)\n",
    "xmin = tf.maximum(reshaped_locals[...,1],xmin)\n",
    "ymax = tf.minimum(reshaped_locals[...,2],ymax)\n",
    "xmax = tf.minimum(reshaped_locals[...,3],xmax)\n",
    "boxes = tf.stack([ymin,xmin,ymax,xmax],axis = -1)\n",
    "\n",
    "#start to remove boxes\n",
    "mask_score = tf.greater_equal(scores,0.5)\n",
    "mask_class = tf.not_equal(classes, 0)\n",
    "total_mask = tf.logical_and(mask_score,mask_class)\n",
    "total_index = tf.where(total_mask[0])\n",
    "#remove background and scores>0.5\n",
    "scores = tf.gather(scores,total_index[:,0],axis=1)\n",
    "classes = tf.gather(classes,total_index[:,0],axis=1)\n",
    "boxes = tf.gather(boxes,total_index[:,0],axis=1)\n",
    "nms_index = tf.image.non_max_suppression(boxes[0], scores[0],50, iou_threshold=0.5)\n",
    "#remove nms boxes\n",
    "scores = tf.gather(scores[0],nms_index,name='scores')\n",
    "classes = tf.gather(classes[0],nms_index,name='classes')\n",
    "boxes = tf.gather(boxes[0],nms_index,name='boxes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(FLAGS.batch_size):\n",
    "    index = (bboxes[i][...,2]-bboxes[i][...,0])>0\n",
    "    temp_label =[]\n",
    "    temp_box = []\n",
    "    for j in range(60):\n",
    "        if(index[j] == True):\n",
    "            temp_label.append(labels[j])\n",
    "            temp_box.append(bboxes[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########need to be modified############################################\n",
    "\n",
    "# for k in range(FLAGS.batch_size):\n",
    "#     loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = temp_logits[k],labels = feat_labels[k])\n",
    "pos_loss,neg_loss,loc_loss = ssd_losses(logits, #预测类别\n",
    "                   localisations,#预测位置\n",
    "                   feat_labels, #ground truth 类别\n",
    "                   feat_localizations, #ground truth 位置\n",
    "                   feat_scores,#ground truth 分数\n",
    "                   match_threshold=0.5,\n",
    "                   negative_ratio=3.,\n",
    "                   alpha=1.,\n",
    "                   label_smoothing=0.,\n",
    "                   scope='ssd_losses')\n",
    "total_losses = pos_loss+neg_loss+loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, annotation = get_path_and_annotation('./VOC2007/train/train_OCR.txt')\n",
    "train_feeder = read_data(img_path,annotation,FLAGS.batch_size,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "def split_cls_anno(annotation_batch):\n",
    "    temp_anno=[]\n",
    "    temp_cls = []\n",
    "    for i in annotation_batch:\n",
    "        temp_anno1=[]\n",
    "        temp_cls1=[]\n",
    "        for j in i:\n",
    "            temp_anno1.append([int(j[0]),int(j[1]),int(j[2]),int(j[3])])#xmin,ymin,xmax,ymax\n",
    "#             temp_anno1.append([int(j[0]),int(j[1]),(int(j[2])-int(j[0])),(int(j[3])-int(j[1]))])\n",
    "            temp_cls1.append(int(j[4]))\n",
    "        temp_anno.append(temp_anno1)\n",
    "        temp_cls.append(temp_cls1)\n",
    "    return temp_anno,temp_cls\n",
    "def data_augmentation(image_batch,annotation_batch,cls_batch): \n",
    "    ###convert xmin,ymin,xmax,ymax to xleft,yleft,w,h to be compatible with tensorlayer format\n",
    "    temp_anno = []\n",
    "    for i in annotation_batch:\n",
    "        temp_anno1=[]\n",
    "        for j in i:\n",
    "            temp_anno1.append([int(j[0]),int(j[1]),(int(j[2])-int(j[0])),(int(j[3])-int(j[1]))])\n",
    "        temp_anno.append(temp_anno1)\n",
    "        \n",
    "    temp_img2 = []\n",
    "    temp_anno2 = []\n",
    "    temp_cls2 = []\n",
    "    for i in range(FLAGS.batch_size):\n",
    "        temp_img1,temp_cls1,temp_anno1 = tl.prepro.obj_box_shift(image_batch[i], cls_batch[i],coords=temp_anno[i],fill_mode='constant',is_rescale=False)\n",
    "        temp_img1,temp_cls1,temp_anno1 = tl.prepro.obj_box_zoom(temp_img1,temp_cls1,coords=temp_anno1,fill_mode='constant',is_rescale=False)\n",
    "        temp_img2.append(temp_img1)\n",
    "        temp_anno2.append(temp_anno1)\n",
    "        temp_cls2.append(temp_cls1)\n",
    "\n",
    "    ##convert x,y,w,h to xmin,ymin,xmax,ymax\n",
    "    temp_anno = []\n",
    "    for i in temp_anno2:\n",
    "        temp_anno1=[]\n",
    "        for j in i:\n",
    "            temp_anno1.append([j[0],j[1],j[2]+j[0],j[3]+j[1]])\n",
    "        temp_anno.append(temp_anno1)\n",
    "    return temp_img2,temp_anno,temp_cls2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_aug_func(image_batch,annotation_batch):\n",
    "###show whether the image and the annotation are compatible after data augmentation\n",
    "    anno_batch = np.array(annotation_batch)*300\n",
    "    for index_img in range(FLAGS.batch_size):\n",
    "        for i in range(len(anno_batch[index_img])):\n",
    "            plt.hlines(int(anno_batch[index_img][i][0]),int(anno_batch[index_img][i][1]),int(anno_batch[index_img][i][3]),colors='red')\n",
    "            plt.hlines(int(anno_batch[index_img][i][2]),int(anno_batch[index_img][i][1]),int(anno_batch[index_img][i][3]),colors='red')\n",
    "            plt.vlines(int(anno_batch[index_img][i][1]),int(anno_batch[index_img][i][0]),int(anno_batch[index_img][i][2]),colors='red')\n",
    "            plt.vlines(int(anno_batch[index_img][i][3]),int(anno_batch[index_img][i][0]),int(anno_batch[index_img][i][2]),colors='red')\n",
    "        plt.imshow(image_batch[index_img])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [image_batch,annotation_batch] = next(train_feeder)\n",
    "# anno_batch,cls_batch = split_cls_anno(annotation_batch)\n",
    "# if(random.random()>0.5):\n",
    "#     image_batch,anno_batch,cls_batch = data_augmentation(image_batch,anno_batch,cls_batch)\n",
    "# image_batch,anno_batch,cls_batch = data_preprocessing(image_batch,anno_batch,cls_batch,FLAGS.image_size)\n",
    "# test_aug_func(image_batch,anno_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "tf.summary.scalar('loss',total_losses)\n",
    "update_opts = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies([tf.group(*update_opts)]):\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(total_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./SSD_Billy/checkpoint/OCR--6000\n",
      "Restore from the checkpoint ./SSD_Billy/checkpoint/OCR--6000\n",
      "Batch: 0   Current total loss: 89.62239 pos_loss: 49.481537 neg_loss: 13.133539 loc_loss: 27.007313\n",
      "INFO:tensorflow:Froze 141 variables.\n",
      "INFO:tensorflow:Converted 141 variables to const ops.\n",
      "Model saved !!!\n",
      "Batch: 1   Current total loss: 69.196785 pos_loss: 37.56295 neg_loss: 11.07383 loc_loss: 20.560005\n",
      "Batch: 2   Current total loss: 66.40064 pos_loss: 38.33261 neg_loss: 11.254723 loc_loss: 16.81331\n",
      "Batch: 3   Current total loss: 88.99429 pos_loss: 49.19397 neg_loss: 13.165367 loc_loss: 26.634958\n",
      "Batch: 4   Current total loss: 77.55124 pos_loss: 38.896046 neg_loss: 11.854639 loc_loss: 26.800556\n",
      "Batch: 5   Current total loss: 80.783424 pos_loss: 43.609238 neg_loss: 11.30732 loc_loss: 25.866867\n",
      "Batch: 6   Current total loss: 68.222206 pos_loss: 34.283813 neg_loss: 11.484272 loc_loss: 22.45412\n",
      "Batch: 7   Current total loss: 62.215393 pos_loss: 31.832573 neg_loss: 9.783981 loc_loss: 20.59884\n",
      "Batch: 8   Current total loss: 82.01669 pos_loss: 41.42363 neg_loss: 14.27255 loc_loss: 26.32051\n",
      "Batch: 9   Current total loss: 82.54422 pos_loss: 46.255486 neg_loss: 12.843685 loc_loss: 23.44505\n",
      "Batch: 10   Current total loss: 83.13275 pos_loss: 40.83275 neg_loss: 13.337835 loc_loss: 28.962162\n",
      "Batch: 11   Current total loss: 62.65177 pos_loss: 30.968946 neg_loss: 11.165848 loc_loss: 20.516977\n",
      "Batch: 12   Current total loss: 51.449173 pos_loss: 25.007797 neg_loss: 11.0725155 loc_loss: 15.368859\n",
      "Batch: 13   Current total loss: 51.287994 pos_loss: 23.837124 neg_loss: 10.702751 loc_loss: 16.748117\n",
      "Batch: 14   Current total loss: 70.87737 pos_loss: 37.849213 neg_loss: 12.023461 loc_loss: 21.0047\n",
      "Batch: 15   Current total loss: 58.57523 pos_loss: 29.516098 neg_loss: 11.446354 loc_loss: 17.612778\n",
      "Batch: 16   Current total loss: 62.293877 pos_loss: 34.11508 neg_loss: 10.78979 loc_loss: 17.389004\n",
      "Batch: 17   Current total loss: 124.3002 pos_loss: 67.30874 neg_loss: 17.147612 loc_loss: 39.843845\n",
      "Batch: 18   Current total loss: 68.15714 pos_loss: 33.198013 neg_loss: 12.868038 loc_loss: 22.09109\n",
      "Batch: 19   Current total loss: 68.353134 pos_loss: 40.220123 neg_loss: 9.7315235 loc_loss: 18.40149\n",
      "Batch: 20   Current total loss: 66.8333 pos_loss: 37.691216 neg_loss: 10.861411 loc_loss: 18.280668\n",
      "Batch: 21   Current total loss: 66.621704 pos_loss: 31.43613 neg_loss: 12.655951 loc_loss: 22.529625\n",
      "Batch: 22   Current total loss: 106.94828 pos_loss: 53.625458 neg_loss: 17.943943 loc_loss: 35.378883\n",
      "Batch: 23   Current total loss: 75.09222 pos_loss: 38.96016 neg_loss: 13.083214 loc_loss: 23.048843\n",
      "Batch: 24   Current total loss: 74.72003 pos_loss: 37.59863 neg_loss: 13.05839 loc_loss: 24.06301\n",
      "Batch: 25   Current total loss: 69.75362 pos_loss: 34.608265 neg_loss: 12.924129 loc_loss: 22.221226\n",
      "Batch: 26   Current total loss: 78.762886 pos_loss: 37.26382 neg_loss: 14.193489 loc_loss: 27.305576\n",
      "Batch: 27   Current total loss: 80.61138 pos_loss: 39.5289 neg_loss: 14.942642 loc_loss: 26.13984\n",
      "Batch: 28   Current total loss: 85.711136 pos_loss: 45.151268 neg_loss: 15.111448 loc_loss: 25.448418\n",
      "Batch: 29   Current total loss: 104.425674 pos_loss: 55.00823 neg_loss: 16.215553 loc_loss: 33.201885\n",
      "Batch: 30   Current total loss: 69.87508 pos_loss: 35.132133 neg_loss: 11.673087 loc_loss: 23.069859\n",
      "Batch: 31   Current total loss: 67.74409 pos_loss: 36.451355 neg_loss: 11.443237 loc_loss: 19.849497\n",
      "Batch: 32   Current total loss: 86.824524 pos_loss: 46.004265 neg_loss: 15.259895 loc_loss: 25.560368\n",
      "Batch: 33   Current total loss: 50.551903 pos_loss: 25.305405 neg_loss: 9.504836 loc_loss: 15.741663\n",
      "Batch: 34   Current total loss: 94.8515 pos_loss: 52.034515 neg_loss: 13.532285 loc_loss: 29.284702\n",
      "Batch: 35   Current total loss: 73.98988 pos_loss: 37.948086 neg_loss: 12.99143 loc_loss: 23.050373\n",
      "Batch: 36   Current total loss: 63.86456 pos_loss: 32.0596 neg_loss: 12.127302 loc_loss: 19.677654\n",
      "Batch: 37   Current total loss: 87.08089 pos_loss: 45.07412 neg_loss: 13.664012 loc_loss: 28.342754\n",
      "Batch: 38   Current total loss: 51.015587 pos_loss: 25.13618 neg_loss: 12.207921 loc_loss: 13.671488\n",
      "Batch: 39   Current total loss: 74.60323 pos_loss: 36.111897 neg_loss: 13.816793 loc_loss: 24.674543\n",
      "Batch: 40   Current total loss: 152.76688 pos_loss: 78.50008 neg_loss: 20.574799 loc_loss: 53.691998\n",
      "Batch: 41   Current total loss: 65.857445 pos_loss: 34.090122 neg_loss: 12.413606 loc_loss: 19.35372\n",
      "Batch: 42   Current total loss: 64.32592 pos_loss: 33.375416 neg_loss: 12.034871 loc_loss: 18.915634\n",
      "Batch: 43   Current total loss: 40.457092 pos_loss: 20.177174 neg_loss: 9.130968 loc_loss: 11.148949\n",
      "Batch: 44   Current total loss: 55.18469 pos_loss: 26.350842 neg_loss: 10.957039 loc_loss: 17.876808\n",
      "Batch: 45   Current total loss: 87.3819 pos_loss: 45.12687 neg_loss: 13.268066 loc_loss: 28.986965\n",
      "Batch: 46   Current total loss: 89.94993 pos_loss: 46.040115 neg_loss: 14.367765 loc_loss: 29.542046\n",
      "Batch: 47   Current total loss: 109.30505 pos_loss: 60.41356 neg_loss: 14.725063 loc_loss: 34.16643\n",
      "Batch: 48   Current total loss: 79.49761 pos_loss: 40.33239 neg_loss: 13.733474 loc_loss: 25.43175\n",
      "Batch: 49   Current total loss: 67.74202 pos_loss: 33.352215 neg_loss: 13.271078 loc_loss: 21.118729\n",
      "Batch: 50   Current total loss: 79.45445 pos_loss: 40.980488 neg_loss: 13.767613 loc_loss: 24.706348\n",
      "Batch: 51   Current total loss: 71.94916 pos_loss: 38.71741 neg_loss: 12.2424965 loc_loss: 20.989252\n",
      "Batch: 52   Current total loss: 69.153145 pos_loss: 36.979774 neg_loss: 12.211033 loc_loss: 19.962337\n",
      "Batch: 53   Current total loss: 92.15474 pos_loss: 45.70793 neg_loss: 17.518208 loc_loss: 28.928602\n",
      "Batch: 54   Current total loss: 65.21087 pos_loss: 35.49675 neg_loss: 12.090127 loc_loss: 17.623993\n",
      "Batch: 55   Current total loss: 58.930565 pos_loss: 30.765179 neg_loss: 10.892197 loc_loss: 17.27319\n",
      "Batch: 56   Current total loss: 94.64253 pos_loss: 46.849342 neg_loss: 17.10113 loc_loss: 30.692062\n",
      "Batch: 57   Current total loss: 69.35644 pos_loss: 37.45145 neg_loss: 11.743262 loc_loss: 20.161726\n",
      "Batch: 58   Current total loss: 63.373978 pos_loss: 32.058693 neg_loss: 11.213983 loc_loss: 20.101301\n",
      "Batch: 59   Current total loss: 79.94095 pos_loss: 43.596348 neg_loss: 12.217246 loc_loss: 24.127354\n",
      "Batch: 60   Current total loss: 82.0776 pos_loss: 43.376015 neg_loss: 13.150948 loc_loss: 25.550636\n",
      "Batch: 61   Current total loss: 62.49768 pos_loss: 28.490358 neg_loss: 13.457954 loc_loss: 20.54937\n",
      "Batch: 62   Current total loss: 82.74776 pos_loss: 43.163574 neg_loss: 13.939749 loc_loss: 25.644436\n",
      "Batch: 63   Current total loss: 97.57768 pos_loss: 49.92313 neg_loss: 16.322693 loc_loss: 31.331861\n",
      "Batch: 64   Current total loss: 51.38366 pos_loss: 23.251345 neg_loss: 10.5166645 loc_loss: 17.615648\n",
      "Batch: 65   Current total loss: 80.87367 pos_loss: 40.226475 neg_loss: 14.231312 loc_loss: 26.41589\n",
      "Batch: 66   Current total loss: 86.35234 pos_loss: 46.849247 neg_loss: 13.173632 loc_loss: 26.32946\n",
      "Batch: 67   Current total loss: 70.73051 pos_loss: 38.13513 neg_loss: 11.306223 loc_loss: 21.289154\n",
      "Batch: 68   Current total loss: 79.24338 pos_loss: 44.68096 neg_loss: 11.938934 loc_loss: 22.62348\n",
      "Batch: 69   Current total loss: 65.50791 pos_loss: 38.344463 neg_loss: 9.439292 loc_loss: 17.724155\n",
      "Batch: 70   Current total loss: 65.815865 pos_loss: 34.49186 neg_loss: 11.708613 loc_loss: 19.61539\n",
      "Batch: 71   Current total loss: 92.19262 pos_loss: 45.605606 neg_loss: 15.842373 loc_loss: 30.74464\n",
      "Batch: 72   Current total loss: 113.05074 pos_loss: 56.686943 neg_loss: 16.226301 loc_loss: 40.137497\n",
      "Batch: 73   Current total loss: 67.06283 pos_loss: 37.465706 neg_loss: 11.093078 loc_loss: 18.504044\n",
      "Batch: 74   Current total loss: 46.86445 pos_loss: 24.309908 neg_loss: 9.445584 loc_loss: 13.108957\n",
      "Batch: 75   Current total loss: 82.774895 pos_loss: 43.90037 neg_loss: 14.45364 loc_loss: 24.420883\n",
      "Batch: 76   Current total loss: 76.99642 pos_loss: 38.273888 neg_loss: 12.707981 loc_loss: 26.014553\n",
      "Batch: 77   Current total loss: 48.109463 pos_loss: 22.577822 neg_loss: 10.7491455 loc_loss: 14.782496\n",
      "Batch: 78   Current total loss: 57.154785 pos_loss: 28.769693 neg_loss: 11.423197 loc_loss: 16.961895\n",
      "Batch: 79   Current total loss: 54.021767 pos_loss: 27.721174 neg_loss: 10.118585 loc_loss: 16.182007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 80   Current total loss: 81.077255 pos_loss: 39.33818 neg_loss: 14.254686 loc_loss: 27.484388\n",
      "Batch: 81   Current total loss: 76.90115 pos_loss: 42.336395 neg_loss: 12.457512 loc_loss: 22.107243\n",
      "Batch: 82   Current total loss: 123.037704 pos_loss: 64.438515 neg_loss: 19.064995 loc_loss: 39.534195\n",
      "Batch: 83   Current total loss: 114.19418 pos_loss: 63.285526 neg_loss: 16.925566 loc_loss: 33.98309\n",
      "Batch: 84   Current total loss: 82.41275 pos_loss: 42.48179 neg_loss: 14.542297 loc_loss: 25.388664\n",
      "Batch: 85   Current total loss: 81.37343 pos_loss: 38.696774 neg_loss: 15.6295395 loc_loss: 27.047112\n",
      "Batch: 86   Current total loss: 76.98892 pos_loss: 36.855556 neg_loss: 14.66167 loc_loss: 25.471693\n",
      "Batch: 87   Current total loss: 44.93946 pos_loss: 23.996805 neg_loss: 9.610575 loc_loss: 11.332081\n",
      "Batch: 88   Current total loss: 104.486694 pos_loss: 54.04097 neg_loss: 17.019035 loc_loss: 33.42669\n",
      "Batch: 89   Current total loss: 90.458084 pos_loss: 48.06028 neg_loss: 15.479527 loc_loss: 26.918278\n",
      "Batch: 90   Current total loss: 66.681305 pos_loss: 33.421925 neg_loss: 12.794407 loc_loss: 20.464977\n",
      "Batch: 91   Current total loss: 88.52452 pos_loss: 47.04403 neg_loss: 13.957995 loc_loss: 27.522495\n",
      "Batch: 92   Current total loss: 55.1345 pos_loss: 25.001701 neg_loss: 12.533201 loc_loss: 17.599596\n",
      "Batch: 93   Current total loss: 81.95361 pos_loss: 41.37538 neg_loss: 15.3758335 loc_loss: 25.202398\n",
      "Batch: 94   Current total loss: 80.87951 pos_loss: 39.736423 neg_loss: 14.843655 loc_loss: 26.299429\n",
      "Batch: 95   Current total loss: 99.4732 pos_loss: 50.101208 neg_loss: 17.864464 loc_loss: 31.507532\n",
      "Batch: 96   Current total loss: 67.65761 pos_loss: 34.7559 neg_loss: 11.507115 loc_loss: 21.394596\n",
      "Batch: 97   Current total loss: 77.88513 pos_loss: 38.707314 neg_loss: 12.765137 loc_loss: 26.412685\n",
      "Batch: 98   Current total loss: 73.53216 pos_loss: 37.705074 neg_loss: 13.255928 loc_loss: 22.571156\n",
      "Batch: 99   Current total loss: 63.357525 pos_loss: 32.47718 neg_loss: 12.069128 loc_loss: 18.811214\n",
      "Batch: 100   Current total loss: 67.857056 pos_loss: 31.84648 neg_loss: 13.168213 loc_loss: 22.84236\n",
      "Batch: 101   Current total loss: 93.41805 pos_loss: 48.249176 neg_loss: 15.123903 loc_loss: 30.044977\n",
      "Batch: 102   Current total loss: 68.702385 pos_loss: 33.42493 neg_loss: 13.187655 loc_loss: 22.089798\n",
      "Batch: 103   Current total loss: 78.971344 pos_loss: 40.501488 neg_loss: 12.61478 loc_loss: 25.855072\n",
      "Batch: 104   Current total loss: 70.87779 pos_loss: 38.34205 neg_loss: 11.975119 loc_loss: 20.560627\n",
      "Batch: 105   Current total loss: 89.24469 pos_loss: 47.255226 neg_loss: 14.869043 loc_loss: 27.120422\n",
      "Batch: 106   Current total loss: 75.891846 pos_loss: 39.46543 neg_loss: 12.974188 loc_loss: 23.452229\n",
      "Batch: 107   Current total loss: 86.989136 pos_loss: 43.575493 neg_loss: 13.735443 loc_loss: 29.678204\n",
      "Batch: 108   Current total loss: 62.850613 pos_loss: 31.773523 neg_loss: 11.797453 loc_loss: 19.279636\n",
      "Batch: 109   Current total loss: 76.243355 pos_loss: 38.665688 neg_loss: 14.562038 loc_loss: 23.015627\n",
      "Batch: 110   Current total loss: 76.57681 pos_loss: 36.76571 neg_loss: 13.35894 loc_loss: 26.452164\n",
      "Batch: 111   Current total loss: 106.30682 pos_loss: 48.33821 neg_loss: 19.097872 loc_loss: 38.870743\n",
      "Batch: 112   Current total loss: 101.745926 pos_loss: 50.657784 neg_loss: 17.847641 loc_loss: 33.2405\n",
      "Batch: 113   Current total loss: 44.188705 pos_loss: 19.53265 neg_loss: 11.504356 loc_loss: 13.151701\n",
      "Batch: 114   Current total loss: 36.68052 pos_loss: 18.017878 neg_loss: 7.836179 loc_loss: 10.826462\n",
      "Batch: 115   Current total loss: 91.72713 pos_loss: 49.873947 neg_loss: 14.524768 loc_loss: 27.328413\n",
      "Batch: 116   Current total loss: 71.729706 pos_loss: 34.438923 neg_loss: 13.438566 loc_loss: 23.852217\n",
      "Batch: 117   Current total loss: 57.709473 pos_loss: 27.367287 neg_loss: 10.552372 loc_loss: 19.789816\n",
      "Batch: 118   Current total loss: 49.854576 pos_loss: 25.340248 neg_loss: 10.030031 loc_loss: 14.484301\n",
      "Batch: 119   Current total loss: 97.92673 pos_loss: 50.643322 neg_loss: 15.901635 loc_loss: 31.381763\n",
      "Batch: 120   Current total loss: 49.9562 pos_loss: 24.409088 neg_loss: 10.792199 loc_loss: 14.754914\n",
      "Batch: 121   Current total loss: 81.52141 pos_loss: 40.907024 neg_loss: 15.826998 loc_loss: 24.78738\n",
      "Batch: 122   Current total loss: 69.95099 pos_loss: 35.282425 neg_loss: 13.003328 loc_loss: 21.66524\n",
      "Batch: 123   Current total loss: 91.30543 pos_loss: 46.527798 neg_loss: 16.11941 loc_loss: 28.658218\n",
      "Batch: 124   Current total loss: 73.32408 pos_loss: 34.984043 neg_loss: 15.1729765 loc_loss: 23.16706\n",
      "Batch: 125   Current total loss: 83.3058 pos_loss: 44.91906 neg_loss: 13.288348 loc_loss: 25.098392\n",
      "Batch: 126   Current total loss: 87.20143 pos_loss: 46.72704 neg_loss: 14.415956 loc_loss: 26.058441\n",
      "Batch: 127   Current total loss: 69.05652 pos_loss: 33.462532 neg_loss: 14.811062 loc_loss: 20.78292\n",
      "Batch: 128   Current total loss: 68.33283 pos_loss: 35.71236 neg_loss: 11.508763 loc_loss: 21.111712\n",
      "Batch: 129   Current total loss: 46.868763 pos_loss: 22.193766 neg_loss: 10.789443 loc_loss: 13.885555\n",
      "Batch: 130   Current total loss: 107.798096 pos_loss: 52.40267 neg_loss: 17.918343 loc_loss: 37.477085\n",
      "Batch: 131   Current total loss: 62.183002 pos_loss: 30.313066 neg_loss: 12.189914 loc_loss: 19.680023\n",
      "Batch: 132   Current total loss: 64.94918 pos_loss: 32.602352 neg_loss: 10.366118 loc_loss: 21.980707\n",
      "Batch: 133   Current total loss: 81.40376 pos_loss: 43.464848 neg_loss: 12.776067 loc_loss: 25.162853\n",
      "Batch: 134   Current total loss: 59.903633 pos_loss: 29.653446 neg_loss: 11.714926 loc_loss: 18.535263\n",
      "Batch: 135   Current total loss: 59.530174 pos_loss: 29.932108 neg_loss: 11.713724 loc_loss: 17.884342\n",
      "Batch: 136   Current total loss: 60.82231 pos_loss: 27.810703 neg_loss: 12.308684 loc_loss: 20.702923\n",
      "Batch: 137   Current total loss: 67.01366 pos_loss: 35.570286 neg_loss: 12.649596 loc_loss: 18.793776\n",
      "Batch: 138   Current total loss: 105.02469 pos_loss: 56.496765 neg_loss: 17.151249 loc_loss: 31.376675\n",
      "Batch: 139   Current total loss: 59.273354 pos_loss: 29.633081 neg_loss: 12.167002 loc_loss: 17.47327\n",
      "Batch: 140   Current total loss: 89.63823 pos_loss: 47.93255 neg_loss: 14.952132 loc_loss: 26.753551\n",
      "Batch: 141   Current total loss: 112.22522 pos_loss: 55.898216 neg_loss: 20.334608 loc_loss: 35.992397\n",
      "Batch: 142   Current total loss: 74.41946 pos_loss: 35.050293 neg_loss: 13.280428 loc_loss: 26.088737\n",
      "Batch: 143   Current total loss: 117.196465 pos_loss: 58.20817 neg_loss: 18.835777 loc_loss: 40.15252\n",
      "Batch: 144   Current total loss: 71.573074 pos_loss: 37.078827 neg_loss: 12.174565 loc_loss: 22.319683\n",
      "Batch: 145   Current total loss: 57.97733 pos_loss: 29.439194 neg_loss: 11.44694 loc_loss: 17.091194\n",
      "Batch: 146   Current total loss: 67.20383 pos_loss: 36.633224 neg_loss: 12.983793 loc_loss: 17.586803\n",
      "Batch: 147   Current total loss: 78.439125 pos_loss: 39.802383 neg_loss: 13.081537 loc_loss: 25.555206\n",
      "Batch: 148   Current total loss: 103.537704 pos_loss: 51.06427 neg_loss: 17.243721 loc_loss: 35.22971\n",
      "Batch: 149   Current total loss: 62.141037 pos_loss: 29.023588 neg_loss: 12.368456 loc_loss: 20.748993\n",
      "Batch: 150   Current total loss: 86.53532 pos_loss: 42.12219 neg_loss: 15.475871 loc_loss: 28.937256\n",
      "Batch: 151   Current total loss: 61.761505 pos_loss: 28.622444 neg_loss: 13.521765 loc_loss: 19.617294\n",
      "Batch: 152   Current total loss: 78.42086 pos_loss: 36.37589 neg_loss: 14.262884 loc_loss: 27.782087\n",
      "Batch: 153   Current total loss: 67.71632 pos_loss: 33.78235 neg_loss: 11.87039 loc_loss: 22.063578\n",
      "Batch: 154   Current total loss: 71.759384 pos_loss: 37.534367 neg_loss: 13.22076 loc_loss: 21.004255\n",
      "Batch: 155   Current total loss: 73.2408 pos_loss: 37.350426 neg_loss: 12.815955 loc_loss: 23.07442\n",
      "Batch: 156   Current total loss: 54.579247 pos_loss: 23.774181 neg_loss: 13.801029 loc_loss: 17.004038\n",
      "Batch: 157   Current total loss: 40.670208 pos_loss: 18.167747 neg_loss: 9.920338 loc_loss: 12.582123\n",
      "Batch: 158   Current total loss: 72.91803 pos_loss: 34.946354 neg_loss: 13.2075405 loc_loss: 24.76414\n",
      "Batch: 159   Current total loss: 50.574326 pos_loss: 23.012634 neg_loss: 11.601517 loc_loss: 15.960174\n",
      "Batch: 160   Current total loss: 76.75429 pos_loss: 41.602966 neg_loss: 12.589539 loc_loss: 22.561783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 161   Current total loss: 66.976036 pos_loss: 32.96934 neg_loss: 12.550634 loc_loss: 21.456062\n",
      "Batch: 162   Current total loss: 65.25969 pos_loss: 31.773296 neg_loss: 12.311502 loc_loss: 21.174885\n",
      "Batch: 163   Current total loss: 75.86702 pos_loss: 36.60206 neg_loss: 14.705458 loc_loss: 24.559502\n",
      "Batch: 164   Current total loss: 58.415497 pos_loss: 29.285196 neg_loss: 10.186981 loc_loss: 18.943321\n",
      "Batch: 165   Current total loss: 52.24426 pos_loss: 22.746368 neg_loss: 10.982 loc_loss: 18.515888\n",
      "Batch: 166   Current total loss: 96.293655 pos_loss: 48.49903 neg_loss: 16.33751 loc_loss: 31.457119\n",
      "Batch: 167   Current total loss: 70.24863 pos_loss: 36.03975 neg_loss: 13.257427 loc_loss: 20.951454\n",
      "Batch: 168   Current total loss: 73.22569 pos_loss: 36.92392 neg_loss: 13.07002 loc_loss: 23.23175\n",
      "Batch: 169   Current total loss: 66.13958 pos_loss: 36.4268 neg_loss: 11.672482 loc_loss: 18.040297\n",
      "Batch: 170   Current total loss: 60.973522 pos_loss: 32.315914 neg_loss: 11.883071 loc_loss: 16.774536\n",
      "Batch: 171   Current total loss: 92.11795 pos_loss: 48.527035 neg_loss: 13.657826 loc_loss: 29.933094\n",
      "Batch: 172   Current total loss: 60.732433 pos_loss: 32.907593 neg_loss: 11.91296 loc_loss: 15.911881\n",
      "Batch: 173   Current total loss: 80.949554 pos_loss: 41.326538 neg_loss: 14.014715 loc_loss: 25.608295\n",
      "Batch: 174   Current total loss: 68.87783 pos_loss: 37.485226 neg_loss: 11.235271 loc_loss: 20.157335\n",
      "Batch: 175   Current total loss: 66.571945 pos_loss: 32.885815 neg_loss: 12.129925 loc_loss: 21.556206\n",
      "Batch: 176   Current total loss: 92.68783 pos_loss: 48.697594 neg_loss: 14.883451 loc_loss: 29.106781\n",
      "Batch: 177   Current total loss: 90.270096 pos_loss: 42.72374 neg_loss: 16.437202 loc_loss: 31.109154\n",
      "Batch: 178   Current total loss: 47.385284 pos_loss: 22.053986 neg_loss: 9.639724 loc_loss: 15.691576\n",
      "Batch: 179   Current total loss: 83.10739 pos_loss: 43.09556 neg_loss: 14.888215 loc_loss: 25.123621\n",
      "Batch: 180   Current total loss: 76.147224 pos_loss: 38.72581 neg_loss: 14.411491 loc_loss: 23.009922\n",
      "Batch: 181   Current total loss: 67.06233 pos_loss: 33.195793 neg_loss: 11.36747 loc_loss: 22.49907\n",
      "Batch: 182   Current total loss: 73.03589 pos_loss: 38.708702 neg_loss: 11.831569 loc_loss: 22.495613\n",
      "Batch: 183   Current total loss: 45.088387 pos_loss: 22.550518 neg_loss: 9.426758 loc_loss: 13.111111\n",
      "Batch: 184   Current total loss: 75.47873 pos_loss: 41.337925 neg_loss: 12.791044 loc_loss: 21.349758\n",
      "Batch: 185   Current total loss: 70.801025 pos_loss: 35.902504 neg_loss: 13.121925 loc_loss: 21.7766\n",
      "Batch: 186   Current total loss: 57.528996 pos_loss: 31.16 neg_loss: 9.70722 loc_loss: 16.661777\n",
      "Batch: 187   Current total loss: 94.084335 pos_loss: 43.789284 neg_loss: 17.869461 loc_loss: 32.42559\n",
      "Batch: 188   Current total loss: 84.68997 pos_loss: 46.575714 neg_loss: 13.215695 loc_loss: 24.898563\n",
      "Batch: 189   Current total loss: 69.0187 pos_loss: 31.661911 neg_loss: 13.767649 loc_loss: 23.58914\n",
      "Batch: 190   Current total loss: 65.446915 pos_loss: 33.685875 neg_loss: 11.945197 loc_loss: 19.815842\n",
      "Batch: 191   Current total loss: 65.13408 pos_loss: 34.544064 neg_loss: 10.7745285 loc_loss: 19.815483\n",
      "Batch: 192   Current total loss: 48.327465 pos_loss: 21.769032 neg_loss: 10.869783 loc_loss: 15.688648\n",
      "Batch: 193   Current total loss: 59.955765 pos_loss: 30.09877 neg_loss: 11.459799 loc_loss: 18.397192\n",
      "Batch: 194   Current total loss: 89.238464 pos_loss: 45.196384 neg_loss: 16.33995 loc_loss: 27.702131\n",
      "Batch: 195   Current total loss: 63.257748 pos_loss: 30.645771 neg_loss: 12.939825 loc_loss: 19.672153\n",
      "Batch: 196   Current total loss: 55.593063 pos_loss: 26.32997 neg_loss: 12.070142 loc_loss: 17.19295\n",
      "Batch: 197   Current total loss: 61.72603 pos_loss: 28.489542 neg_loss: 12.037141 loc_loss: 21.199343\n",
      "Batch: 198   Current total loss: 63.120003 pos_loss: 31.97198 neg_loss: 11.61489 loc_loss: 19.533136\n",
      "Batch: 199   Current total loss: 42.38439 pos_loss: 18.925905 neg_loss: 10.674595 loc_loss: 12.78389\n",
      "Batch: 200   Current total loss: 48.803097 pos_loss: 24.20127 neg_loss: 10.002421 loc_loss: 14.599406\n",
      "Batch: 201   Current total loss: 61.744858 pos_loss: 31.382751 neg_loss: 10.917069 loc_loss: 19.445036\n",
      "Batch: 202   Current total loss: 74.420975 pos_loss: 37.685467 neg_loss: 12.741493 loc_loss: 23.994015\n",
      "Batch: 203   Current total loss: 65.18617 pos_loss: 30.927063 neg_loss: 13.04431 loc_loss: 21.214796\n",
      "Batch: 204   Current total loss: 76.40927 pos_loss: 41.085686 neg_loss: 13.3283205 loc_loss: 21.995262\n",
      "Batch: 205   Current total loss: 67.07299 pos_loss: 34.63798 neg_loss: 12.603322 loc_loss: 19.831688\n",
      "Batch: 206   Current total loss: 75.88886 pos_loss: 40.291294 neg_loss: 12.31824 loc_loss: 23.279325\n",
      "Batch: 207   Current total loss: 62.812634 pos_loss: 33.35299 neg_loss: 10.057276 loc_loss: 19.40237\n",
      "Batch: 208   Current total loss: 84.10244 pos_loss: 41.580803 neg_loss: 14.548328 loc_loss: 27.973307\n",
      "Batch: 209   Current total loss: 128.73505 pos_loss: 65.7845 neg_loss: 18.334925 loc_loss: 44.615616\n",
      "Batch: 210   Current total loss: 71.570274 pos_loss: 36.790222 neg_loss: 13.176981 loc_loss: 21.603071\n",
      "Batch: 211   Current total loss: 46.996178 pos_loss: 20.826612 neg_loss: 10.56299 loc_loss: 15.606573\n",
      "Batch: 212   Current total loss: 77.5323 pos_loss: 39.693817 neg_loss: 13.291397 loc_loss: 24.547089\n",
      "Batch: 213   Current total loss: 83.83855 pos_loss: 39.525642 neg_loss: 14.537558 loc_loss: 29.775341\n",
      "Batch: 214   Current total loss: 45.438137 pos_loss: 24.425882 neg_loss: 8.854186 loc_loss: 12.158071\n",
      "Batch: 215   Current total loss: 73.82254 pos_loss: 39.99264 neg_loss: 12.269485 loc_loss: 21.560417\n",
      "Batch: 216   Current total loss: 65.61015 pos_loss: 36.198654 neg_loss: 11.060759 loc_loss: 18.350742\n",
      "Batch: 217   Current total loss: 52.63669 pos_loss: 27.591934 neg_loss: 9.641377 loc_loss: 15.403379\n",
      "Batch: 218   Current total loss: 86.33118 pos_loss: 47.201107 neg_loss: 14.094002 loc_loss: 25.036066\n",
      "Batch: 219   Current total loss: 60.518238 pos_loss: 31.78648 neg_loss: 10.64282 loc_loss: 18.08894\n",
      "Batch: 220   Current total loss: 98.39891 pos_loss: 49.850662 neg_loss: 15.760122 loc_loss: 32.788124\n",
      "Batch: 221   Current total loss: 71.718636 pos_loss: 38.02409 neg_loss: 12.56139 loc_loss: 21.133156\n",
      "Batch: 222   Current total loss: 83.37808 pos_loss: 43.858437 neg_loss: 14.473959 loc_loss: 25.045685\n",
      "Batch: 223   Current total loss: 79.25865 pos_loss: 39.361492 neg_loss: 14.943107 loc_loss: 24.95405\n",
      "Batch: 224   Current total loss: 94.78824 pos_loss: 50.095764 neg_loss: 14.936567 loc_loss: 29.755907\n",
      "Batch: 225   Current total loss: 87.09823 pos_loss: 43.859497 neg_loss: 13.887846 loc_loss: 29.350883\n",
      "Batch: 226   Current total loss: 77.25138 pos_loss: 40.73305 neg_loss: 13.708742 loc_loss: 22.809586\n",
      "Batch: 227   Current total loss: 95.53065 pos_loss: 48.204826 neg_loss: 16.050783 loc_loss: 31.27504\n",
      "Batch: 228   Current total loss: 72.39825 pos_loss: 40.20144 neg_loss: 12.724427 loc_loss: 19.472382\n",
      "Batch: 229   Current total loss: 54.433086 pos_loss: 25.169699 neg_loss: 11.304956 loc_loss: 17.958431\n",
      "Batch: 230   Current total loss: 63.7149 pos_loss: 35.668133 neg_loss: 10.756521 loc_loss: 17.290249\n",
      "Batch: 231   Current total loss: 62.502747 pos_loss: 31.015339 neg_loss: 12.137627 loc_loss: 19.349781\n",
      "Batch: 232   Current total loss: 46.483665 pos_loss: 21.395786 neg_loss: 10.236574 loc_loss: 14.851304\n",
      "Batch: 233   Current total loss: 57.15548 pos_loss: 27.61004 neg_loss: 12.719353 loc_loss: 16.826086\n",
      "Batch: 234   Current total loss: 70.11797 pos_loss: 35.015736 neg_loss: 12.112419 loc_loss: 22.989819\n",
      "Batch: 235   Current total loss: 61.036716 pos_loss: 32.2294 neg_loss: 11.253598 loc_loss: 17.553719\n",
      "Batch: 236   Current total loss: 54.910057 pos_loss: 27.315714 neg_loss: 11.4392605 loc_loss: 16.155083\n",
      "Batch: 237   Current total loss: 52.682236 pos_loss: 25.991875 neg_loss: 9.908344 loc_loss: 16.782019\n",
      "Batch: 238   Current total loss: 51.815132 pos_loss: 24.10372 neg_loss: 12.493832 loc_loss: 15.217583\n",
      "Batch: 239   Current total loss: 69.48315 pos_loss: 35.227432 neg_loss: 11.968529 loc_loss: 22.287186\n",
      "Batch: 240   Current total loss: 90.77665 pos_loss: 46.725887 neg_loss: 14.220615 loc_loss: 29.830149\n",
      "Batch: 241   Current total loss: 49.481937 pos_loss: 23.951447 neg_loss: 9.898727 loc_loss: 15.631763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 242   Current total loss: 84.80569 pos_loss: 43.032307 neg_loss: 14.097737 loc_loss: 27.675646\n",
      "Batch: 243   Current total loss: 72.952255 pos_loss: 39.079704 neg_loss: 13.5714855 loc_loss: 20.301067\n",
      "Batch: 244   Current total loss: 55.243652 pos_loss: 29.001678 neg_loss: 11.300584 loc_loss: 14.941389\n",
      "Batch: 245   Current total loss: 92.9991 pos_loss: 46.305176 neg_loss: 16.214825 loc_loss: 30.479103\n",
      "Batch: 246   Current total loss: 43.67484 pos_loss: 19.072723 neg_loss: 10.182182 loc_loss: 14.419933\n",
      "Batch: 247   Current total loss: 57.01858 pos_loss: 26.276794 neg_loss: 11.974293 loc_loss: 18.767494\n",
      "Batch: 248   Current total loss: 75.778496 pos_loss: 40.183773 neg_loss: 12.643701 loc_loss: 22.951023\n",
      "Batch: 249   Current total loss: 100.71057 pos_loss: 51.122658 neg_loss: 16.600208 loc_loss: 32.9877\n",
      "Batch: 250   Current total loss: 84.9966 pos_loss: 41.010998 neg_loss: 14.490461 loc_loss: 29.495142\n",
      "Batch: 251   Current total loss: 87.86108 pos_loss: 44.90435 neg_loss: 14.767557 loc_loss: 28.18917\n",
      "Batch: 252   Current total loss: 83.77051 pos_loss: 41.33223 neg_loss: 15.232428 loc_loss: 27.205845\n",
      "Batch: 253   Current total loss: 72.88316 pos_loss: 36.430824 neg_loss: 12.727477 loc_loss: 23.724861\n",
      "Batch: 254   Current total loss: 50.20253 pos_loss: 24.74264 neg_loss: 10.671923 loc_loss: 14.787968\n",
      "Batch: 255   Current total loss: 58.955544 pos_loss: 27.174192 neg_loss: 11.887508 loc_loss: 19.893845\n",
      "Batch: 256   Current total loss: 45.60218 pos_loss: 20.218616 neg_loss: 11.174676 loc_loss: 14.208889\n",
      "Batch: 257   Current total loss: 73.695114 pos_loss: 36.950188 neg_loss: 13.406836 loc_loss: 23.338089\n",
      "Batch: 258   Current total loss: 54.533794 pos_loss: 24.898218 neg_loss: 12.883693 loc_loss: 16.751884\n",
      "Batch: 259   Current total loss: 131.94385 pos_loss: 69.73262 neg_loss: 19.702526 loc_loss: 42.508698\n",
      "Batch: 260   Current total loss: 64.10513 pos_loss: 32.00535 neg_loss: 9.738803 loc_loss: 22.360981\n",
      "Batch: 261   Current total loss: 63.005344 pos_loss: 31.473627 neg_loss: 12.557701 loc_loss: 18.974018\n",
      "Batch: 262   Current total loss: 48.564842 pos_loss: 22.902843 neg_loss: 10.970638 loc_loss: 14.691359\n",
      "Batch: 263   Current total loss: 114.896904 pos_loss: 59.43023 neg_loss: 16.787436 loc_loss: 38.679237\n",
      "Batch: 264   Current total loss: 84.5699 pos_loss: 40.398685 neg_loss: 14.906807 loc_loss: 29.264408\n",
      "Batch: 265   Current total loss: 79.88329 pos_loss: 40.401283 neg_loss: 15.351705 loc_loss: 24.130306\n",
      "Batch: 266   Current total loss: 64.443306 pos_loss: 32.125374 neg_loss: 11.433574 loc_loss: 20.884356\n",
      "Batch: 267   Current total loss: 90.421104 pos_loss: 48.561256 neg_loss: 13.377636 loc_loss: 28.482212\n",
      "Batch: 268   Current total loss: 59.663685 pos_loss: 31.861046 neg_loss: 10.594807 loc_loss: 17.207832\n",
      "Batch: 269   Current total loss: 56.05784 pos_loss: 28.534908 neg_loss: 10.054409 loc_loss: 17.468521\n",
      "Batch: 270   Current total loss: 76.31889 pos_loss: 38.989464 neg_loss: 13.803007 loc_loss: 23.52642\n",
      "Batch: 271   Current total loss: 54.484177 pos_loss: 27.95562 neg_loss: 11.47332 loc_loss: 15.055239\n",
      "Batch: 272   Current total loss: 74.52265 pos_loss: 34.788017 neg_loss: 14.4261265 loc_loss: 25.308512\n",
      "Batch: 273   Current total loss: 69.39729 pos_loss: 35.354446 neg_loss: 12.3750105 loc_loss: 21.667835\n",
      "Batch: 274   Current total loss: 85.718124 pos_loss: 45.06347 neg_loss: 14.086356 loc_loss: 26.568295\n",
      "Batch: 275   Current total loss: 93.25934 pos_loss: 46.03219 neg_loss: 16.863722 loc_loss: 30.363426\n",
      "Batch: 276   Current total loss: 59.9495 pos_loss: 28.796883 neg_loss: 13.49309 loc_loss: 17.65953\n",
      "Batch: 277   Current total loss: 85.36896 pos_loss: 40.03196 neg_loss: 15.410713 loc_loss: 29.926289\n",
      "Batch: 278   Current total loss: 61.69311 pos_loss: 29.408623 neg_loss: 12.485664 loc_loss: 19.798824\n",
      "Batch: 279   Current total loss: 82.538086 pos_loss: 41.713177 neg_loss: 15.860716 loc_loss: 24.964191\n",
      "Batch: 280   Current total loss: 58.056183 pos_loss: 26.311405 neg_loss: 11.063709 loc_loss: 20.681068\n",
      "Batch: 281   Current total loss: 67.49109 pos_loss: 36.5127 neg_loss: 11.089111 loc_loss: 19.889275\n",
      "Batch: 282   Current total loss: 61.786243 pos_loss: 30.896933 neg_loss: 12.648367 loc_loss: 18.240944\n",
      "Batch: 283   Current total loss: 99.14042 pos_loss: 54.130775 neg_loss: 15.515055 loc_loss: 29.49459\n",
      "Batch: 284   Current total loss: 47.832546 pos_loss: 26.506336 neg_loss: 9.0700865 loc_loss: 12.256123\n",
      "Batch: 285   Current total loss: 64.34055 pos_loss: 32.4185 neg_loss: 12.773136 loc_loss: 19.148916\n",
      "Batch: 286   Current total loss: 79.11806 pos_loss: 37.882328 neg_loss: 15.449373 loc_loss: 25.78635\n",
      "Batch: 287   Current total loss: 84.48044 pos_loss: 42.25255 neg_loss: 15.775187 loc_loss: 26.45271\n",
      "Batch: 288   Current total loss: 91.94838 pos_loss: 46.742332 neg_loss: 14.7409315 loc_loss: 30.46511\n",
      "Batch: 289   Current total loss: 59.78878 pos_loss: 29.264462 neg_loss: 10.896705 loc_loss: 19.627615\n",
      "Batch: 290   Current total loss: 49.702057 pos_loss: 22.990932 neg_loss: 10.259225 loc_loss: 16.4519\n",
      "Batch: 291   Current total loss: 66.285446 pos_loss: 33.343777 neg_loss: 12.4554 loc_loss: 20.48627\n",
      "Batch: 292   Current total loss: 103.70146 pos_loss: 56.050774 neg_loss: 15.318245 loc_loss: 32.33244\n",
      "Batch: 293   Current total loss: 58.22213 pos_loss: 30.851784 neg_loss: 10.643666 loc_loss: 16.726683\n",
      "Batch: 294   Current total loss: 70.731064 pos_loss: 32.687996 neg_loss: 14.457621 loc_loss: 23.585451\n",
      "Batch: 295   Current total loss: 62.353085 pos_loss: 34.567055 neg_loss: 10.664579 loc_loss: 17.121449\n",
      "Batch: 296   Current total loss: 67.61035 pos_loss: 34.658264 neg_loss: 12.457725 loc_loss: 20.494366\n",
      "Batch: 297   Current total loss: 67.2767 pos_loss: 33.39576 neg_loss: 13.381628 loc_loss: 20.49931\n",
      "Batch: 298   Current total loss: 100.46383 pos_loss: 52.55024 neg_loss: 15.652061 loc_loss: 32.261528\n",
      "Batch: 299   Current total loss: 93.755714 pos_loss: 50.861755 neg_loss: 14.405476 loc_loss: 28.488487\n",
      "Batch: 300   Current total loss: 91.00586 pos_loss: 47.924522 neg_loss: 15.180592 loc_loss: 27.900742\n",
      "Batch: 301   Current total loss: 89.157074 pos_loss: 43.903336 neg_loss: 15.230195 loc_loss: 30.023544\n",
      "Batch: 302   Current total loss: 99.92784 pos_loss: 54.47579 neg_loss: 14.990069 loc_loss: 30.461979\n",
      "Batch: 303   Current total loss: 86.00505 pos_loss: 45.49064 neg_loss: 13.407568 loc_loss: 27.106838\n",
      "Batch: 304   Current total loss: 63.37816 pos_loss: 32.57896 neg_loss: 11.741015 loc_loss: 19.058182\n",
      "Batch: 305   Current total loss: 84.66002 pos_loss: 45.928825 neg_loss: 13.507439 loc_loss: 25.223759\n",
      "Batch: 306   Current total loss: 77.25266 pos_loss: 38.443848 neg_loss: 14.996842 loc_loss: 23.811975\n",
      "Batch: 307   Current total loss: 65.78587 pos_loss: 32.66693 neg_loss: 12.804132 loc_loss: 20.314814\n",
      "Batch: 308   Current total loss: 65.798485 pos_loss: 33.94974 neg_loss: 12.633926 loc_loss: 19.214819\n",
      "Batch: 309   Current total loss: 110.54998 pos_loss: 59.473137 neg_loss: 17.062452 loc_loss: 34.01439\n",
      "Batch: 310   Current total loss: 60.143948 pos_loss: 30.567993 neg_loss: 10.871788 loc_loss: 18.704166\n",
      "Batch: 311   Current total loss: 76.31337 pos_loss: 36.199455 neg_loss: 14.638568 loc_loss: 25.47535\n",
      "Batch: 312   Current total loss: 93.51883 pos_loss: 49.38024 neg_loss: 15.474779 loc_loss: 28.663815\n",
      "Batch: 313   Current total loss: 65.10385 pos_loss: 32.259117 neg_loss: 12.616721 loc_loss: 20.228016\n",
      "Batch: 314   Current total loss: 86.41684 pos_loss: 44.81809 neg_loss: 15.880244 loc_loss: 25.718508\n",
      "Batch: 315   Current total loss: 56.264214 pos_loss: 30.700243 neg_loss: 11.135355 loc_loss: 14.428617\n",
      "Batch: 316   Current total loss: 71.54613 pos_loss: 38.467113 neg_loss: 12.543176 loc_loss: 20.535841\n",
      "Batch: 317   Current total loss: 61.027138 pos_loss: 30.306831 neg_loss: 11.883287 loc_loss: 18.837019\n",
      "Batch: 318   Current total loss: 82.38855 pos_loss: 42.08754 neg_loss: 12.996911 loc_loss: 27.304104\n",
      "Batch: 319   Current total loss: 57.469017 pos_loss: 29.148361 neg_loss: 10.925825 loc_loss: 17.394827\n",
      "Batch: 320   Current total loss: 51.40513 pos_loss: 24.077164 neg_loss: 10.95986 loc_loss: 16.368101\n",
      "Batch: 321   Current total loss: 61.513046 pos_loss: 31.561792 neg_loss: 11.528473 loc_loss: 18.422783\n",
      "Batch: 322   Current total loss: 100.57767 pos_loss: 53.199432 neg_loss: 16.650742 loc_loss: 30.727497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 323   Current total loss: 81.69571 pos_loss: 40.69918 neg_loss: 14.313991 loc_loss: 26.682533\n",
      "Batch: 324   Current total loss: 56.14097 pos_loss: 27.156004 neg_loss: 11.1118965 loc_loss: 17.873068\n",
      "Batch: 325   Current total loss: 41.58114 pos_loss: 19.104109 neg_loss: 10.2452965 loc_loss: 12.231735\n",
      "Batch: 326   Current total loss: 49.276222 pos_loss: 24.247168 neg_loss: 11.048037 loc_loss: 13.98102\n",
      "Batch: 327   Current total loss: 55.590267 pos_loss: 27.311869 neg_loss: 11.571335 loc_loss: 16.707066\n",
      "Batch: 328   Current total loss: 58.85713 pos_loss: 27.66961 neg_loss: 12.837869 loc_loss: 18.349651\n",
      "Batch: 329   Current total loss: 51.018124 pos_loss: 29.008524 neg_loss: 9.352968 loc_loss: 12.656632\n",
      "Batch: 330   Current total loss: 106.2487 pos_loss: 56.10235 neg_loss: 17.419788 loc_loss: 32.726562\n",
      "Batch: 331   Current total loss: 75.78421 pos_loss: 36.213657 neg_loss: 14.574968 loc_loss: 24.99558\n",
      "Batch: 332   Current total loss: 47.015938 pos_loss: 24.620832 neg_loss: 8.614192 loc_loss: 13.780913\n",
      "Batch: 333   Current total loss: 66.04709 pos_loss: 34.220352 neg_loss: 11.352006 loc_loss: 20.474735\n",
      "Batch: 334   Current total loss: 70.80812 pos_loss: 29.702263 neg_loss: 15.770634 loc_loss: 25.335224\n",
      "Batch: 335   Current total loss: 96.0148 pos_loss: 46.91103 neg_loss: 17.206211 loc_loss: 31.897564\n",
      "Batch: 336   Current total loss: 70.98028 pos_loss: 34.311043 neg_loss: 14.360012 loc_loss: 22.309223\n",
      "Batch: 337   Current total loss: 72.20621 pos_loss: 36.006096 neg_loss: 13.678262 loc_loss: 22.521854\n",
      "Batch: 338   Current total loss: 73.56972 pos_loss: 37.354668 neg_loss: 14.113543 loc_loss: 22.10151\n",
      "Batch: 339   Current total loss: 82.962814 pos_loss: 38.055534 neg_loss: 15.920401 loc_loss: 28.986877\n",
      "Batch: 340   Current total loss: 61.74951 pos_loss: 28.259378 neg_loss: 13.344274 loc_loss: 20.145857\n",
      "Batch: 341   Current total loss: 79.16471 pos_loss: 40.24575 neg_loss: 14.5438385 loc_loss: 24.375122\n",
      "Batch: 342   Current total loss: 99.266815 pos_loss: 52.459343 neg_loss: 16.37201 loc_loss: 30.43546\n",
      "Batch: 343   Current total loss: 80.22544 pos_loss: 38.475388 neg_loss: 15.452146 loc_loss: 26.297909\n",
      "Batch: 344   Current total loss: 58.959694 pos_loss: 29.433401 neg_loss: 11.318266 loc_loss: 18.208027\n",
      "Batch: 345   Current total loss: 56.484283 pos_loss: 26.780966 neg_loss: 11.905717 loc_loss: 17.7976\n",
      "Batch: 346   Current total loss: 65.12587 pos_loss: 33.42841 neg_loss: 12.631763 loc_loss: 19.065697\n",
      "Batch: 347   Current total loss: 81.080055 pos_loss: 42.35437 neg_loss: 12.932653 loc_loss: 25.793032\n",
      "Batch: 348   Current total loss: 87.44473 pos_loss: 44.77704 neg_loss: 15.039792 loc_loss: 27.627907\n",
      "Batch: 349   Current total loss: 57.650032 pos_loss: 26.078579 neg_loss: 12.809532 loc_loss: 18.761923\n",
      "Batch: 350   Current total loss: 70.615036 pos_loss: 35.244812 neg_loss: 12.052648 loc_loss: 23.317575\n",
      "Batch: 351   Current total loss: 61.05555 pos_loss: 31.998644 neg_loss: 10.961071 loc_loss: 18.095835\n",
      "Batch: 352   Current total loss: 69.877174 pos_loss: 32.10431 neg_loss: 16.311611 loc_loss: 21.461256\n",
      "Batch: 353   Current total loss: 57.606243 pos_loss: 24.998951 neg_loss: 11.745592 loc_loss: 20.861698\n",
      "Batch: 354   Current total loss: 108.407 pos_loss: 57.834248 neg_loss: 18.602478 loc_loss: 31.970272\n",
      "Batch: 355   Current total loss: 68.16542 pos_loss: 33.67191 neg_loss: 13.012973 loc_loss: 21.480541\n",
      "Batch: 356   Current total loss: 44.92328 pos_loss: 22.84245 neg_loss: 10.209728 loc_loss: 11.871102\n",
      "Batch: 357   Current total loss: 71.86925 pos_loss: 34.286976 neg_loss: 12.667922 loc_loss: 24.914345\n",
      "Batch: 358   Current total loss: 56.50656 pos_loss: 30.28718 neg_loss: 10.2702465 loc_loss: 15.949137\n",
      "Batch: 359   Current total loss: 110.24823 pos_loss: 60.658497 neg_loss: 15.202942 loc_loss: 34.3868\n",
      "Batch: 360   Current total loss: 86.59714 pos_loss: 43.59783 neg_loss: 15.537895 loc_loss: 27.46141\n",
      "Batch: 361   Current total loss: 65.18007 pos_loss: 30.281952 neg_loss: 11.768219 loc_loss: 23.129896\n",
      "Batch: 362   Current total loss: 76.04823 pos_loss: 39.8462 neg_loss: 14.038773 loc_loss: 22.163265\n",
      "Batch: 363   Current total loss: 55.03482 pos_loss: 26.138258 neg_loss: 11.261068 loc_loss: 17.635492\n",
      "Batch: 364   Current total loss: 79.47768 pos_loss: 39.934753 neg_loss: 13.199726 loc_loss: 26.3432\n",
      "Batch: 365   Current total loss: 64.15341 pos_loss: 35.192013 neg_loss: 11.856976 loc_loss: 17.10442\n",
      "Batch: 366   Current total loss: 86.677765 pos_loss: 44.478428 neg_loss: 15.361744 loc_loss: 26.837593\n",
      "Batch: 367   Current total loss: 54.524998 pos_loss: 27.240063 neg_loss: 11.75892 loc_loss: 15.526017\n",
      "Batch: 368   Current total loss: 63.312546 pos_loss: 33.12219 neg_loss: 11.468316 loc_loss: 18.722044\n",
      "Batch: 369   Current total loss: 59.61285 pos_loss: 31.306507 neg_loss: 10.90435 loc_loss: 17.401993\n",
      "Batch: 370   Current total loss: 72.7944 pos_loss: 37.377052 neg_loss: 12.911648 loc_loss: 22.505705\n",
      "Batch: 371   Current total loss: 82.14024 pos_loss: 42.903446 neg_loss: 13.954243 loc_loss: 25.282555\n",
      "Batch: 372   Current total loss: 75.297844 pos_loss: 39.469666 neg_loss: 11.9389105 loc_loss: 23.889269\n",
      "Batch: 373   Current total loss: 56.167694 pos_loss: 29.516077 neg_loss: 10.324368 loc_loss: 16.327248\n",
      "Batch: 374   Current total loss: 77.80342 pos_loss: 38.865826 neg_loss: 14.828796 loc_loss: 24.108799\n",
      "Batch: 375   Current total loss: 73.71027 pos_loss: 36.0559 neg_loss: 13.150707 loc_loss: 24.503666\n",
      "Batch: 376   Current total loss: 98.27627 pos_loss: 50.467445 neg_loss: 16.215633 loc_loss: 31.593193\n",
      "Batch: 377   Current total loss: 85.51778 pos_loss: 44.38154 neg_loss: 13.4219475 loc_loss: 27.714294\n",
      "Batch: 378   Current total loss: 55.513268 pos_loss: 27.4099 neg_loss: 11.450616 loc_loss: 16.652752\n",
      "Batch: 379   Current total loss: 77.76121 pos_loss: 39.99713 neg_loss: 13.65573 loc_loss: 24.108347\n",
      "Batch: 380   Current total loss: 68.487915 pos_loss: 37.02225 neg_loss: 11.051123 loc_loss: 20.414545\n",
      "Batch: 381   Current total loss: 92.99595 pos_loss: 46.75432 neg_loss: 18.081015 loc_loss: 28.160614\n",
      "Batch: 382   Current total loss: 56.72655 pos_loss: 31.563782 neg_loss: 10.280525 loc_loss: 14.882244\n",
      "Batch: 383   Current total loss: 74.75734 pos_loss: 36.48102 neg_loss: 13.352547 loc_loss: 24.923775\n",
      "Batch: 384   Current total loss: 66.63151 pos_loss: 33.989708 neg_loss: 12.288122 loc_loss: 20.353678\n",
      "Batch: 385   Current total loss: 99.01465 pos_loss: 50.326897 neg_loss: 16.29645 loc_loss: 32.3913\n",
      "Batch: 386   Current total loss: 56.19283 pos_loss: 28.847305 neg_loss: 9.968493 loc_loss: 17.37703\n",
      "Batch: 387   Current total loss: 45.34898 pos_loss: 24.352852 neg_loss: 9.16595 loc_loss: 11.830178\n",
      "Batch: 388   Current total loss: 89.26257 pos_loss: 46.03595 neg_loss: 16.324123 loc_loss: 26.902502\n",
      "Batch: 389   Current total loss: 56.306374 pos_loss: 28.799004 neg_loss: 10.231421 loc_loss: 17.27595\n",
      "Batch: 390   Current total loss: 63.78639 pos_loss: 30.55035 neg_loss: 11.314678 loc_loss: 21.92136\n",
      "Batch: 391   Current total loss: 57.38142 pos_loss: 28.93362 neg_loss: 11.600864 loc_loss: 16.846935\n",
      "Batch: 392   Current total loss: 43.81976 pos_loss: 22.086796 neg_loss: 8.856943 loc_loss: 12.8760195\n",
      "Batch: 393   Current total loss: 72.64607 pos_loss: 34.486183 neg_loss: 14.380078 loc_loss: 23.779808\n",
      "Batch: 394   Current total loss: 58.260727 pos_loss: 30.252762 neg_loss: 11.21372 loc_loss: 16.794243\n",
      "Batch: 395   Current total loss: 87.31192 pos_loss: 44.922825 neg_loss: 15.116266 loc_loss: 27.272825\n",
      "Batch: 396   Current total loss: 145.40062 pos_loss: 79.29397 neg_loss: 20.601028 loc_loss: 45.50562\n",
      "Batch: 397   Current total loss: 70.95262 pos_loss: 36.68934 neg_loss: 12.925278 loc_loss: 21.338009\n",
      "Batch: 398   Current total loss: 101.92569 pos_loss: 48.409912 neg_loss: 17.182999 loc_loss: 36.332775\n",
      "Batch: 399   Current total loss: 59.40852 pos_loss: 27.33107 neg_loss: 12.356764 loc_loss: 19.720684\n",
      "Batch: 400   Current total loss: 51.836403 pos_loss: 25.495798 neg_loss: 11.500507 loc_loss: 14.840097\n",
      "Batch: 401   Current total loss: 92.30563 pos_loss: 49.453857 neg_loss: 14.866599 loc_loss: 27.985176\n",
      "Batch: 402   Current total loss: 109.74487 pos_loss: 55.930233 neg_loss: 18.134632 loc_loss: 35.680008\n",
      "Batch: 403   Current total loss: 67.69554 pos_loss: 34.646255 neg_loss: 12.345648 loc_loss: 20.70364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 404   Current total loss: 83.84166 pos_loss: 42.006493 neg_loss: 15.650294 loc_loss: 26.184868\n",
      "Batch: 405   Current total loss: 67.469376 pos_loss: 31.35815 neg_loss: 13.538634 loc_loss: 22.57259\n",
      "Batch: 406   Current total loss: 90.28976 pos_loss: 44.163925 neg_loss: 17.648516 loc_loss: 28.477318\n",
      "Batch: 407   Current total loss: 72.83812 pos_loss: 34.89322 neg_loss: 14.654341 loc_loss: 23.290562\n",
      "Batch: 408   Current total loss: 96.01322 pos_loss: 46.54315 neg_loss: 18.236317 loc_loss: 31.233757\n",
      "Batch: 409   Current total loss: 61.807503 pos_loss: 31.273693 neg_loss: 10.548139 loc_loss: 19.985672\n",
      "Batch: 410   Current total loss: 72.29126 pos_loss: 36.907608 neg_loss: 11.669362 loc_loss: 23.714287\n",
      "Batch: 411   Current total loss: 67.60034 pos_loss: 35.351288 neg_loss: 12.768734 loc_loss: 19.480324\n",
      "Batch: 412   Current total loss: 39.884064 pos_loss: 21.926506 neg_loss: 9.012698 loc_loss: 8.944859\n",
      "Batch: 413   Current total loss: 87.66872 pos_loss: 40.55926 neg_loss: 14.945772 loc_loss: 32.163677\n",
      "Batch: 414   Current total loss: 77.816376 pos_loss: 41.299545 neg_loss: 13.37891 loc_loss: 23.137918\n",
      "Batch: 415   Current total loss: 61.6342 pos_loss: 28.336937 neg_loss: 12.776855 loc_loss: 20.520409\n",
      "Batch: 416   Current total loss: 89.658875 pos_loss: 46.82919 neg_loss: 13.41506 loc_loss: 29.414623\n",
      "Batch: 417   Current total loss: 63.587967 pos_loss: 33.62233 neg_loss: 10.580814 loc_loss: 19.384823\n",
      "Batch: 418   Current total loss: 72.06435 pos_loss: 35.247566 neg_loss: 13.699488 loc_loss: 23.11729\n",
      "Batch: 419   Current total loss: 77.15249 pos_loss: 39.108547 neg_loss: 13.765615 loc_loss: 24.27833\n",
      "Batch: 420   Current total loss: 68.12688 pos_loss: 36.036232 neg_loss: 11.69065 loc_loss: 20.39999\n",
      "Batch: 421   Current total loss: 80.27803 pos_loss: 40.028706 neg_loss: 14.049406 loc_loss: 26.19992\n",
      "Batch: 422   Current total loss: 77.51411 pos_loss: 39.321407 neg_loss: 14.626736 loc_loss: 23.565962\n",
      "Batch: 423   Current total loss: 76.43493 pos_loss: 36.968628 neg_loss: 13.25194 loc_loss: 26.214361\n",
      "Batch: 424   Current total loss: 85.73932 pos_loss: 39.7994 neg_loss: 16.301424 loc_loss: 29.6385\n",
      "Batch: 425   Current total loss: 95.33269 pos_loss: 47.39307 neg_loss: 16.367813 loc_loss: 31.571802\n",
      "Batch: 426   Current total loss: 56.119144 pos_loss: 25.692577 neg_loss: 13.651882 loc_loss: 16.774685\n",
      "Batch: 427   Current total loss: 44.37407 pos_loss: 22.273582 neg_loss: 9.004791 loc_loss: 13.095696\n",
      "Batch: 428   Current total loss: 60.672096 pos_loss: 30.931776 neg_loss: 10.782408 loc_loss: 18.95791\n",
      "Batch: 429   Current total loss: 81.5128 pos_loss: 41.47074 neg_loss: 13.932266 loc_loss: 26.10979\n",
      "Batch: 430   Current total loss: 53.696617 pos_loss: 25.929993 neg_loss: 10.947153 loc_loss: 16.819473\n",
      "Batch: 431   Current total loss: 50.055183 pos_loss: 23.45086 neg_loss: 10.747667 loc_loss: 15.856661\n",
      "Batch: 432   Current total loss: 46.105057 pos_loss: 21.544262 neg_loss: 10.5214615 loc_loss: 14.039331\n",
      "Batch: 433   Current total loss: 89.73563 pos_loss: 47.33873 neg_loss: 14.052418 loc_loss: 28.34448\n",
      "Batch: 434   Current total loss: 85.01381 pos_loss: 41.11651 neg_loss: 17.712742 loc_loss: 26.184559\n",
      "Batch: 435   Current total loss: 70.74084 pos_loss: 33.55794 neg_loss: 13.730538 loc_loss: 23.452358\n",
      "Batch: 436   Current total loss: 79.079155 pos_loss: 38.76285 neg_loss: 15.480046 loc_loss: 24.836258\n",
      "Batch: 437   Current total loss: 58.69208 pos_loss: 28.726952 neg_loss: 13.098017 loc_loss: 16.867115\n",
      "Batch: 438   Current total loss: 99.31729 pos_loss: 47.405533 neg_loss: 16.891092 loc_loss: 35.020664\n",
      "Batch: 439   Current total loss: 81.60246 pos_loss: 43.40126 neg_loss: 14.183597 loc_loss: 24.017609\n",
      "Batch: 440   Current total loss: 78.524345 pos_loss: 38.099625 neg_loss: 15.76366 loc_loss: 24.66106\n",
      "Batch: 441   Current total loss: 53.913685 pos_loss: 24.898113 neg_loss: 11.2423315 loc_loss: 17.77324\n",
      "Batch: 442   Current total loss: 40.523468 pos_loss: 20.575657 neg_loss: 8.924744 loc_loss: 11.023068\n",
      "Batch: 443   Current total loss: 76.53999 pos_loss: 34.971508 neg_loss: 15.508604 loc_loss: 26.059887\n",
      "Batch: 444   Current total loss: 57.89268 pos_loss: 28.653374 neg_loss: 11.524849 loc_loss: 17.714458\n",
      "Batch: 445   Current total loss: 57.893513 pos_loss: 28.809193 neg_loss: 11.431501 loc_loss: 17.65282\n",
      "Batch: 446   Current total loss: 87.120865 pos_loss: 46.581978 neg_loss: 14.065962 loc_loss: 26.472927\n",
      "Batch: 447   Current total loss: 42.784874 pos_loss: 21.130178 neg_loss: 9.254355 loc_loss: 12.400343\n",
      "Batch: 448   Current total loss: 45.807278 pos_loss: 23.836685 neg_loss: 9.504371 loc_loss: 12.466222\n",
      "Batch: 449   Current total loss: 56.97045 pos_loss: 26.022326 neg_loss: 11.736782 loc_loss: 19.211342\n",
      "Batch: 450   Current total loss: 88.27162 pos_loss: 45.622772 neg_loss: 14.711964 loc_loss: 27.936882\n",
      "Batch: 451   Current total loss: 71.29593 pos_loss: 39.230675 neg_loss: 12.288026 loc_loss: 19.777233\n",
      "Batch: 452   Current total loss: 58.85206 pos_loss: 28.52763 neg_loss: 12.174747 loc_loss: 18.149681\n",
      "Batch: 453   Current total loss: 82.636505 pos_loss: 40.755707 neg_loss: 15.257887 loc_loss: 26.62291\n",
      "Batch: 454   Current total loss: 88.64889 pos_loss: 43.060226 neg_loss: 16.858084 loc_loss: 28.730577\n",
      "Batch: 455   Current total loss: 77.51204 pos_loss: 36.573425 neg_loss: 14.439763 loc_loss: 26.498856\n",
      "Batch: 456   Current total loss: 74.96976 pos_loss: 37.21665 neg_loss: 13.09973 loc_loss: 24.653381\n",
      "Batch: 457   Current total loss: 92.80317 pos_loss: 46.23539 neg_loss: 15.512726 loc_loss: 31.055054\n",
      "Batch: 458   Current total loss: 54.060097 pos_loss: 26.281605 neg_loss: 11.039537 loc_loss: 16.738955\n",
      "Batch: 459   Current total loss: 52.769676 pos_loss: 28.92171 neg_loss: 10.278386 loc_loss: 13.56958\n",
      "Batch: 460   Current total loss: 91.52937 pos_loss: 47.25928 neg_loss: 14.649009 loc_loss: 29.621086\n",
      "Batch: 461   Current total loss: 98.73895 pos_loss: 49.290848 neg_loss: 16.380604 loc_loss: 33.067505\n",
      "Batch: 462   Current total loss: 61.233704 pos_loss: 28.563238 neg_loss: 12.592471 loc_loss: 20.077993\n",
      "Batch: 463   Current total loss: 65.70928 pos_loss: 33.250423 neg_loss: 11.538464 loc_loss: 20.920393\n",
      "Batch: 464   Current total loss: 37.88112 pos_loss: 17.457006 neg_loss: 9.546017 loc_loss: 10.878094\n",
      "Batch: 465   Current total loss: 86.244095 pos_loss: 38.976486 neg_loss: 15.913239 loc_loss: 31.35437\n",
      "Batch: 466   Current total loss: 54.842384 pos_loss: 25.968523 neg_loss: 10.665527 loc_loss: 18.208336\n",
      "Batch: 467   Current total loss: 71.14473 pos_loss: 37.197876 neg_loss: 13.257168 loc_loss: 20.689686\n",
      "Batch: 468   Current total loss: 61.311558 pos_loss: 29.728163 neg_loss: 12.220497 loc_loss: 19.362896\n",
      "Batch: 469   Current total loss: 50.39836 pos_loss: 24.26461 neg_loss: 10.382957 loc_loss: 15.750795\n",
      "Batch: 470   Current total loss: 52.854473 pos_loss: 24.970524 neg_loss: 12.545193 loc_loss: 15.338757\n",
      "Batch: 471   Current total loss: 67.184 pos_loss: 31.727161 neg_loss: 12.637487 loc_loss: 22.819351\n",
      "Batch: 472   Current total loss: 57.728058 pos_loss: 28.515415 neg_loss: 11.907171 loc_loss: 17.305475\n",
      "Batch: 473   Current total loss: 88.26553 pos_loss: 46.457287 neg_loss: 14.2324095 loc_loss: 27.575836\n",
      "Batch: 474   Current total loss: 73.79829 pos_loss: 36.466743 neg_loss: 13.69824 loc_loss: 23.6333\n",
      "Batch: 475   Current total loss: 62.92066 pos_loss: 29.507433 neg_loss: 12.672937 loc_loss: 20.740288\n",
      "Batch: 476   Current total loss: 66.98309 pos_loss: 32.614395 neg_loss: 13.556206 loc_loss: 20.812489\n",
      "Batch: 477   Current total loss: 62.802402 pos_loss: 30.999372 neg_loss: 10.810086 loc_loss: 20.992943\n",
      "Batch: 478   Current total loss: 45.32878 pos_loss: 18.852789 neg_loss: 10.211472 loc_loss: 16.264519\n",
      "Batch: 479   Current total loss: 95.2888 pos_loss: 47.523457 neg_loss: 16.678997 loc_loss: 31.08635\n",
      "Batch: 480   Current total loss: 63.738342 pos_loss: 33.686226 neg_loss: 11.952164 loc_loss: 18.099953\n",
      "Batch: 481   Current total loss: 69.84192 pos_loss: 34.882847 neg_loss: 12.829508 loc_loss: 22.129559\n",
      "Batch: 482   Current total loss: 58.572247 pos_loss: 30.049963 neg_loss: 11.131584 loc_loss: 17.390697\n",
      "Batch: 483   Current total loss: 69.4709 pos_loss: 37.602646 neg_loss: 12.726166 loc_loss: 19.14209\n",
      "Batch: 484   Current total loss: 74.36393 pos_loss: 38.64068 neg_loss: 11.674427 loc_loss: 24.048826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 485   Current total loss: 93.3303 pos_loss: 48.394806 neg_loss: 16.49788 loc_loss: 28.437614\n",
      "Batch: 486   Current total loss: 66.451965 pos_loss: 32.535812 neg_loss: 12.768059 loc_loss: 21.14809\n",
      "Batch: 487   Current total loss: 58.739883 pos_loss: 32.221497 neg_loss: 10.360068 loc_loss: 16.158316\n",
      "Batch: 488   Current total loss: 65.80395 pos_loss: 34.98018 neg_loss: 11.060583 loc_loss: 19.763184\n",
      "Batch: 489   Current total loss: 93.80365 pos_loss: 49.529106 neg_loss: 14.553515 loc_loss: 29.721033\n",
      "Batch: 490   Current total loss: 68.37135 pos_loss: 33.15762 neg_loss: 13.842711 loc_loss: 21.371017\n",
      "Batch: 491   Current total loss: 54.410652 pos_loss: 26.74206 neg_loss: 11.305384 loc_loss: 16.363207\n",
      "Batch: 492   Current total loss: 54.19674 pos_loss: 27.04227 neg_loss: 11.558609 loc_loss: 15.595859\n",
      "Batch: 493   Current total loss: 100.8327 pos_loss: 52.024185 neg_loss: 17.24998 loc_loss: 31.55853\n",
      "Batch: 494   Current total loss: 84.88823 pos_loss: 42.84957 neg_loss: 13.407888 loc_loss: 28.630768\n",
      "Batch: 495   Current total loss: 52.927246 pos_loss: 25.346252 neg_loss: 11.0085 loc_loss: 16.572495\n",
      "Batch: 496   Current total loss: 60.932514 pos_loss: 30.650517 neg_loss: 10.636536 loc_loss: 19.645462\n",
      "Batch: 497   Current total loss: 81.304474 pos_loss: 43.67636 neg_loss: 12.97153 loc_loss: 24.656582\n",
      "Batch: 498   Current total loss: 67.01156 pos_loss: 33.289024 neg_loss: 12.803642 loc_loss: 20.918894\n",
      "Batch: 499   Current total loss: 55.86884 pos_loss: 29.068607 neg_loss: 10.264989 loc_loss: 16.535244\n",
      "Batch: 500   Current total loss: 65.511635 pos_loss: 32.646935 neg_loss: 13.091846 loc_loss: 19.772854\n",
      "INFO:tensorflow:Froze 141 variables.\n",
      "INFO:tensorflow:Converted 141 variables to const ops.\n",
      "Model saved !!!\n",
      "Batch: 501   Current total loss: 82.60333 pos_loss: 46.11766 neg_loss: 12.427526 loc_loss: 24.058142\n",
      "Batch: 502   Current total loss: 71.71907 pos_loss: 34.608017 neg_loss: 14.1115055 loc_loss: 22.999546\n",
      "Batch: 503   Current total loss: 69.753334 pos_loss: 34.166805 neg_loss: 12.738865 loc_loss: 22.847664\n",
      "Batch: 504   Current total loss: 65.73775 pos_loss: 37.062984 neg_loss: 10.412658 loc_loss: 18.262102\n",
      "Batch: 505   Current total loss: 45.126953 pos_loss: 20.20687 neg_loss: 10.542891 loc_loss: 14.377193\n",
      "Batch: 506   Current total loss: 54.81514 pos_loss: 27.566187 neg_loss: 10.971056 loc_loss: 16.277897\n",
      "Batch: 507   Current total loss: 100.67145 pos_loss: 51.731262 neg_loss: 18.083937 loc_loss: 30.85625\n",
      "Batch: 508   Current total loss: 56.630775 pos_loss: 26.741173 neg_loss: 12.764623 loc_loss: 17.124983\n",
      "Batch: 509   Current total loss: 55.59158 pos_loss: 25.803892 neg_loss: 12.518125 loc_loss: 17.269562\n",
      "Batch: 510   Current total loss: 59.767197 pos_loss: 28.288147 neg_loss: 11.298197 loc_loss: 20.180855\n",
      "Batch: 511   Current total loss: 58.83725 pos_loss: 28.697899 neg_loss: 11.134111 loc_loss: 19.005241\n",
      "Batch: 512   Current total loss: 38.559597 pos_loss: 17.952051 neg_loss: 9.598309 loc_loss: 11.009237\n",
      "Batch: 513   Current total loss: 39.17861 pos_loss: 18.17884 neg_loss: 9.730552 loc_loss: 11.269217\n",
      "Batch: 514   Current total loss: 70.52538 pos_loss: 34.694244 neg_loss: 12.542496 loc_loss: 23.288643\n",
      "Batch: 515   Current total loss: 70.35771 pos_loss: 35.499195 neg_loss: 12.595152 loc_loss: 22.263365\n",
      "Batch: 516   Current total loss: 80.835686 pos_loss: 37.099007 neg_loss: 15.37788 loc_loss: 28.3588\n",
      "Batch: 517   Current total loss: 70.03105 pos_loss: 37.35147 neg_loss: 13.279312 loc_loss: 19.40027\n",
      "Batch: 518   Current total loss: 68.16614 pos_loss: 33.92324 neg_loss: 11.953897 loc_loss: 22.289003\n",
      "Batch: 519   Current total loss: 55.07531 pos_loss: 29.974651 neg_loss: 10.140996 loc_loss: 14.9596615\n",
      "Batch: 520   Current total loss: 64.76135 pos_loss: 33.45356 neg_loss: 10.867319 loc_loss: 20.440475\n",
      "Batch: 521   Current total loss: 73.66188 pos_loss: 39.05404 neg_loss: 11.213856 loc_loss: 23.393982\n",
      "Batch: 522   Current total loss: 125.21788 pos_loss: 63.493732 neg_loss: 19.243357 loc_loss: 42.480785\n",
      "Batch: 523   Current total loss: 73.457504 pos_loss: 38.859238 neg_loss: 12.854713 loc_loss: 21.743557\n",
      "Batch: 524   Current total loss: 51.169346 pos_loss: 24.932148 neg_loss: 10.414379 loc_loss: 15.822818\n",
      "Batch: 525   Current total loss: 76.238075 pos_loss: 38.748062 neg_loss: 13.51796 loc_loss: 23.972054\n",
      "Batch: 526   Current total loss: 46.75626 pos_loss: 22.972168 neg_loss: 9.443356 loc_loss: 14.340735\n",
      "Batch: 527   Current total loss: 70.45531 pos_loss: 35.088352 neg_loss: 11.908166 loc_loss: 23.45879\n",
      "Batch: 528   Current total loss: 63.497765 pos_loss: 34.98685 neg_loss: 11.28137 loc_loss: 17.229548\n",
      "Batch: 529   Current total loss: 71.24121 pos_loss: 37.649796 neg_loss: 11.970066 loc_loss: 21.621351\n",
      "Batch: 530   Current total loss: 50.387333 pos_loss: 26.788078 neg_loss: 9.180152 loc_loss: 14.419102\n",
      "Batch: 531   Current total loss: 88.8789 pos_loss: 45.90721 neg_loss: 14.393967 loc_loss: 28.57772\n",
      "Batch: 532   Current total loss: 54.266655 pos_loss: 29.098927 neg_loss: 10.33613 loc_loss: 14.831595\n",
      "Batch: 533   Current total loss: 85.26009 pos_loss: 44.156452 neg_loss: 13.641537 loc_loss: 27.462103\n",
      "Batch: 534   Current total loss: 70.321434 pos_loss: 37.055172 neg_loss: 12.112126 loc_loss: 21.154135\n",
      "Batch: 535   Current total loss: 84.6855 pos_loss: 45.08335 neg_loss: 14.845474 loc_loss: 24.756672\n",
      "Batch: 536   Current total loss: 79.509575 pos_loss: 39.228104 neg_loss: 15.334926 loc_loss: 24.946545\n",
      "Batch: 537   Current total loss: 70.59391 pos_loss: 37.83423 neg_loss: 12.262499 loc_loss: 20.497187\n",
      "Batch: 538   Current total loss: 80.78352 pos_loss: 42.370857 neg_loss: 12.939842 loc_loss: 25.47282\n",
      "Batch: 539   Current total loss: 106.29173 pos_loss: 52.225143 neg_loss: 17.104198 loc_loss: 36.96239\n",
      "Batch: 540   Current total loss: 96.673035 pos_loss: 47.591118 neg_loss: 16.135725 loc_loss: 32.94619\n",
      "Batch: 541   Current total loss: 79.6007 pos_loss: 43.616413 neg_loss: 14.050052 loc_loss: 21.934237\n",
      "Batch: 542   Current total loss: 66.04373 pos_loss: 31.940197 neg_loss: 12.338578 loc_loss: 21.76496\n",
      "Batch: 543   Current total loss: 63.635822 pos_loss: 33.90161 neg_loss: 11.312607 loc_loss: 18.421604\n",
      "Batch: 544   Current total loss: 58.052563 pos_loss: 27.002491 neg_loss: 12.785402 loc_loss: 18.264668\n",
      "Batch: 545   Current total loss: 36.761856 pos_loss: 16.679205 neg_loss: 9.595087 loc_loss: 10.487564\n",
      "Batch: 546   Current total loss: 79.69325 pos_loss: 37.39774 neg_loss: 16.750381 loc_loss: 25.54513\n",
      "Batch: 547   Current total loss: 82.98866 pos_loss: 43.710213 neg_loss: 12.958498 loc_loss: 26.31995\n",
      "Batch: 548   Current total loss: 52.217693 pos_loss: 26.426868 neg_loss: 10.551218 loc_loss: 15.239605\n",
      "Batch: 549   Current total loss: 66.64073 pos_loss: 33.20577 neg_loss: 13.407616 loc_loss: 20.027348\n",
      "Batch: 550   Current total loss: 48.155586 pos_loss: 22.921776 neg_loss: 9.900159 loc_loss: 15.3336525\n",
      "Batch: 551   Current total loss: 38.345966 pos_loss: 15.500366 neg_loss: 11.309085 loc_loss: 11.536513\n",
      "Batch: 552   Current total loss: 84.417015 pos_loss: 43.78089 neg_loss: 14.580996 loc_loss: 26.055128\n",
      "Batch: 553   Current total loss: 75.73458 pos_loss: 38.244972 neg_loss: 12.705008 loc_loss: 24.784601\n",
      "Batch: 554   Current total loss: 61.96263 pos_loss: 30.4454 neg_loss: 11.43561 loc_loss: 20.08162\n",
      "Batch: 555   Current total loss: 92.1478 pos_loss: 47.21281 neg_loss: 15.632084 loc_loss: 29.302902\n",
      "Batch: 556   Current total loss: 67.79149 pos_loss: 36.03862 neg_loss: 13.0881195 loc_loss: 18.664745\n",
      "Batch: 557   Current total loss: 54.63423 pos_loss: 28.50681 neg_loss: 10.821267 loc_loss: 15.306154\n",
      "Batch: 558   Current total loss: 72.62193 pos_loss: 37.751366 neg_loss: 13.05205 loc_loss: 21.818518\n",
      "Batch: 559   Current total loss: 44.570877 pos_loss: 19.623955 neg_loss: 10.690662 loc_loss: 14.25626\n",
      "Batch: 560   Current total loss: 56.366974 pos_loss: 27.510267 neg_loss: 12.330377 loc_loss: 16.52633\n",
      "Batch: 561   Current total loss: 66.58261 pos_loss: 34.577686 neg_loss: 12.715543 loc_loss: 19.28938\n",
      "Batch: 562   Current total loss: 99.48999 pos_loss: 51.007034 neg_loss: 16.821306 loc_loss: 31.661648\n",
      "Batch: 563   Current total loss: 76.68173 pos_loss: 37.678143 neg_loss: 13.200257 loc_loss: 25.803331\n",
      "Batch: 564   Current total loss: 77.38913 pos_loss: 38.37581 neg_loss: 13.705641 loc_loss: 25.307674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 565   Current total loss: 89.77366 pos_loss: 44.538532 neg_loss: 16.588642 loc_loss: 28.646484\n",
      "Batch: 566   Current total loss: 57.220314 pos_loss: 26.214296 neg_loss: 11.931675 loc_loss: 19.07434\n",
      "Batch: 567   Current total loss: 62.667038 pos_loss: 32.052834 neg_loss: 11.203445 loc_loss: 19.410759\n",
      "Batch: 568   Current total loss: 57.917442 pos_loss: 26.582607 neg_loss: 11.495725 loc_loss: 19.839111\n",
      "Batch: 569   Current total loss: 39.34737 pos_loss: 16.460726 neg_loss: 10.677675 loc_loss: 12.20897\n",
      "Batch: 570   Current total loss: 57.41555 pos_loss: 30.260784 neg_loss: 10.989801 loc_loss: 16.164963\n",
      "Batch: 571   Current total loss: 59.89202 pos_loss: 28.757816 neg_loss: 13.256561 loc_loss: 17.877644\n",
      "Batch: 572   Current total loss: 99.14696 pos_loss: 52.836365 neg_loss: 16.020323 loc_loss: 30.290268\n",
      "Batch: 573   Current total loss: 60.88717 pos_loss: 31.775572 neg_loss: 9.957527 loc_loss: 19.15407\n",
      "Batch: 574   Current total loss: 63.04441 pos_loss: 31.684752 neg_loss: 12.299416 loc_loss: 19.06024\n",
      "Batch: 575   Current total loss: 43.2968 pos_loss: 19.716948 neg_loss: 10.830213 loc_loss: 12.749638\n",
      "Batch: 576   Current total loss: 99.27465 pos_loss: 49.76941 neg_loss: 15.245434 loc_loss: 34.259804\n",
      "Batch: 577   Current total loss: 61.19127 pos_loss: 29.049791 neg_loss: 12.283295 loc_loss: 19.858185\n",
      "Batch: 578   Current total loss: 95.82186 pos_loss: 48.272987 neg_loss: 17.32116 loc_loss: 30.227715\n",
      "Batch: 579   Current total loss: 63.54299 pos_loss: 32.859344 neg_loss: 10.710471 loc_loss: 19.973177\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.log_dir,sess.graph)\n",
    "    saver = tf.train.Saver(max_to_keep=10)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "    if ckpt:\n",
    "        saver.restore(sess,ckpt)\n",
    "        print('Restore from the checkpoint {0}'.format(ckpt))\n",
    "    else:\n",
    "        print('Train ssd from start')\n",
    "    for i in range(50000):\n",
    "#         [image_batch,annotation_batch] = next(train_feeder)\n",
    "        [image_batch,annotation_batch] = next(train_feeder)\n",
    "        anno_batch,cls_batch = split_cls_anno(annotation_batch)\n",
    "        if(random.random()>0.5):\n",
    "            image_batch,anno_batch,cls_batch = data_augmentation(image_batch,anno_batch,cls_batch)\n",
    "        image_batch,anno_batch,cls_batch = data_preprocessing(image_batch,anno_batch,cls_batch,FLAGS.image_size)       \n",
    "        [_,temp_loss,temp_pos_loss,temp_neg_loss,temp_loc_loss,temp_label,temp_score,temp_loc] = sess.run([train_opt,total_losses,pos_loss,neg_loss,loc_loss,feat_labels,feat_scores,feat_localizations],feed_dict={inputs:image_batch,bboxes:anno_batch,labels:cls_batch,is_training: True})\n",
    "        print('Batch: '+str(i)+'   Current total loss: '+str(temp_loss)+' pos_loss: '+str(temp_pos_loss)+' neg_loss: '+str(temp_neg_loss)+' loc_loss: '+str(temp_loc_loss))\n",
    "        if(temp_loss==0):\n",
    "            break\n",
    "        if(i%500==0):\n",
    "            #save model for training\n",
    "            saver.save(sess,os.path.join(FLAGS.checkpoint_dir,'OCR-'),global_step=i) \n",
    "            ##########################save model for inference#############################################\n",
    "            constant_graph = convert_variables_to_constants(sess, sess.graph_def, ['boxes','scores','classes'])\n",
    "            with tf.gfile.FastGFile('./model.pb', mode='wb') as f:\n",
    "                f.write(constant_graph.SerializeToString())\n",
    "            ######################################################################\n",
    "            print('Model saved !!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ymin = y-h/2\n",
    "ymax = y+h/2\n",
    "xmin = x-w/2\n",
    "xmax = x+w/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_with_anchors(bbox,ymin,xmin,ymax,xmax):\n",
    "    \"\"\"Compute jaccard score between a box and the anchors.\n",
    "    \"\"\"\n",
    "    int_ymin = np.maximum(ymin, bbox[0])\n",
    "    int_xmin = np.maximum(xmin, bbox[1])\n",
    "    int_ymax = np.minimum(ymax, bbox[2])\n",
    "    int_xmax = np.minimum(xmax, bbox[3])\n",
    "    h = np.maximum(int_ymax - int_ymin, 0.)\n",
    "    w = np.maximum(int_xmax - int_xmin, 0.)\n",
    "    # Volumes.\n",
    "    inter_vol = h * w\n",
    "    vol_anchors = (xmax - xmin) * (ymax - ymin)\n",
    "    union_vol = vol_anchors - inter_vol \\\n",
    "        + (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n",
    "    jaccard = np.divide(inter_vol, union_vol)\n",
    "    return jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_with_anchors(anno_batch[0][0],ymin,xmin,ymax,xmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.hlines(anno_batch[0][...,0]*300,anno_batch[0][...,1]*300,anno_batch[0][...,3]*300,colors='red')\n",
    "plt.hlines(anno_batch[0][...,2]*300,anno_batch[0][...,1]*300,anno_batch[0][...,3]*300,colors='red')\n",
    "plt.vlines(anno_batch[0][...,1]*300,anno_batch[0][...,0]*300,anno_batch[0][...,2]*300,colors='red')\n",
    "plt.vlines(anno_batch[0][...,3]*300,anno_batch[0][...,0]*300,anno_batch[0][...,2]*300,colors='red')\n",
    "\n",
    "plt.imshow(image_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
